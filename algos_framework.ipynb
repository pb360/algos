{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "140dc2e8-f7a3-4aba-8924-30029c679156",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div id='outline_algos_framework'> </div> \n",
    "\n",
    "# outline \n",
    "\n",
    "* [to do](#todo)\n",
    "* [imports](#imports)\n",
    "* Training pipeline and walk through \n",
    "    * [setup](#setup)\n",
    "        * [save and load key objects](#HANDLING__SIGNAL_DICT__MODEL__FRAMEWORK_RESULTS__PIPELINE)\n",
    "            * `signal_dict`, pytorch `model`s, `framework_results`, `gs_results`\n",
    "        * [run parameters](#run_parameters)\n",
    "            * create `one_run_params` from `grid_search_params` \n",
    "        * [feature and target <b> formation </b>](#target_and_feature_formation) \n",
    "            * create `prices` DataFrame from trade level data in clickhouse\n",
    "            * crypto quant data processing\n",
    "            * [minute granularity info from `prices`](#minute_granularity_features_from_prices)\n",
    "                * features created using `make_feature_set()` which is defined via `print_feature_making_code(desired_features_dict)`\n",
    "            * [avellaneda stoikov](#avellaneda_stoikov_mm) feature creation\n",
    "        * [feature and target <b> ingestion </b>](#target_and_feature_ingestion) - bring in features / targets from existing files\n",
    "        * [check `prices` and `x` made or read in above](#check_prices_and_features)\n",
    "        * [example target production](#example_target_production)\n",
    "            * plot targets / new expirmental stuff with workarounds in `plot_framework()`\n",
    "    * [separate model run, each part done individually](#separate_model_run) \n",
    "        * [model training and prediction](#model_train_and_preds)\n",
    "        * [decision algorithm](#decision_algorithm) \n",
    "            * $g(f(x), prices) = action$\n",
    "            * given model output of model trained on a rolling basis, $\\hat{Y} \\in \\mathbb{R} ~[-1, 1]$ and the price history $P$ figure out what the transactions would have been. \n",
    "            * plotting the value of a portfolio\n",
    "    * [grid search](#grid_search) over [targets, model, decision] params\n",
    "        * prior gridsearch runs read in [setup](#setup)'s [run parameters](#run_parameters) section\n",
    "        * set current run's `gridsearch_params`\n",
    "    * [one model run](#one_model_run) runs once off [targets, model, decision] params\n",
    "        * run whole process off `one_run_params` created in [setup](#setup)'s [run parameters](#run_parameters) section\n",
    "    * [neural net DEV](#neural_net_dev)\n",
    "        * [MLP](#neural_net_mlp)\n",
    "            * [batched](#mlp_batched)\n",
    "            * [rolling](#mlp_rolling) \n",
    "        * [LSTM](#neural_net_lstm)\n",
    "* [**production** notes on how things are currently ran](#production_instruction) \n",
    "* [**future** development and live functionality ](#live_functionality) development --- there is a living suboutline maintained in this section\n",
    "    [continious live development scratch space](#live_dev_scratch_space)\n",
    "* [avellaneda stoikov](#avellaneda_stoikov_mm)\n",
    "* [copy pasta](#copy_pasta) \n",
    "    * [sql console](#sql_console_copypasta)\n",
    "\n",
    "#### [deving_here](#hereherehere)\n",
    "<div id=''> </div>\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed3dc29-140f-4f8f-8a69-7ff56e3f4521",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div id='todo'> </div>\n",
    "\n",
    "# to do\n",
    "\n",
    "* store this clickhouse command for remote access somewhere\n",
    "    * `clickhouse-client --host=76.124.240.4 --port=8123 --user=default --password`\n",
    "\n",
    "* go through this list and clean out trash, lots of things that are no longer relevant\n",
    "* /algos/machine_specific/config_machine_specific.py should be merged into /algos/config.py, kindof big so do later\n",
    "    * security check, everything machine specific should be isolated to the .env file. \n",
    "* check dot_env usage, with new init_<object> approach out of utils shouldn't need it all over \n",
    "* get a good clickhouse GUI \n",
    "* annual study of threshold based strategy \n",
    "* \n",
    "* # dockerize [update_trading_summary, update_signal, live_framework]\n",
    "    * update_trading_summary\n",
    "        * \n",
    "    * update_signal\n",
    "        * \n",
    "    * live_framework\n",
    "* # dashboard\n",
    "* # add liquidation maps \n",
    "    * transactions (pushed to clickhouse for every trade executed (just do literally every trade for now if they happen in multiple, because they are small I doubt they would \n",
    "* add port value from positions table in plotting_dashboard.py\n",
    "    * \n",
    "* `def make_vwap_dict(feature_params']`\n",
    "* add `p_time_held` to `gs_df` \n",
    "    * this is to include in potential ranking metrics. \n",
    "* in the decision algorithm make a new switch for `d_signal` the derivative of signal \n",
    "    * if the signal is slowing down \n",
    "* in update `update_trading_summary.py`\n",
    "    * make multi ticker... currently it is work,ing for BTC-USDT, add BTC-TUSD and ETH-TUSD \n",
    "    * if there is a long period of time missing (i.e. more than a day)\n",
    "        * get the trades day by day to convert to trading summary and handle from there... too much data read to do a month of trades if needed for whatever reason (adding a new ticker for example) \n",
    "* for desired feature dict, I think its finally required to make a function that actually writes the code for calculations to its own file for doing. \n",
    "    * `from algos.make_feature_set_printed_fn import make_feature_set_printed_fn ` \n",
    "    * SEE COMMENT: # ###PAUL TODO: this should be depricated by code which prints a python file that does the calculations \n",
    "    * WITH THAT BEING SAID, this would only be to support different `desired_feature_dicts`... this would not be for speeding up calculations at each iteration\n",
    "* reconcilliation of closed orders\n",
    "* get new signal working from dev2... to do the `state_dict` --> \\[`state` and `data`\\] dicts mentioned below, going to be a hell of alot easier to spot errors and what not with a new signal running. This will allow a fresh initialization of a `state_dict` for a strategy so that we can see what isn't included anymore\n",
    "    * \n",
    "* split `state_dict` removing all non-jsonable objects and putting them in  `data_dict`, which the goal of would be to only have data indexed in it  \n",
    "* add multi asset support to state_dict... will need to be extended to `data_dict` \n",
    "    * `state_dict['bullish']` --> `state_dict[ticker]['bullish']`\n",
    "* on `state_dict['order_open_dict']`, this should be checked in the live bot, it seems that some very old orders are sticking around, new ones are getting deleted... \n",
    "* update model / signal generation utilizing grid search data.  \n",
    "* when ordering print out the state dictionary \n",
    "* add a \"time of day\" feature which should go with the most granular data \n",
    "    * don't think this will be terribly useful now but maybe absolutley necessary when adding non 24/7 market data \n",
    "    * one liner for this: \n",
    "        * `df['time_scaled'] = (x['timestamp'].dt.hour + x['timestamp'].dt.minute/60) / 24`\n",
    "* `feature_params`\n",
    "    * merging data of multiple time scales and data sources \n",
    "        * need a new method for this, the existing one is unreasonably slow.. maybe best to use a numpy array.. its as simple as forward filling less granular observations till there is another one\n",
    "    * try to optimize compatibality with `one_run_params['reatures']`\n",
    "        * big to do to get done here (though since getting trade only based model off the ground first this is less concerning) \n",
    "* `ch_client` ---- utils should rely on fewer `ch_client`s\n",
    "    * generating one each function call is not remotely efficient, but doing this requires process mapping \n",
    "* order generation / afiguring still needs to be adjusted for ###PAUL_usd_denomination \n",
    "    * its working on `state_dict['bag_desired'][pair]['base_in_quote']` when it should probably be using a combination of newly added value representations \n",
    "* `update_signal` dockerized --  volume issue fixed\n",
    "* #### in `live_framework_v2.py` \n",
    "    * clean up initialization of `state_dict` and filling of `params`, put them in their own wrapper functions\n",
    "    * on <b>order tracking </b> via `open_orders.csv`\n",
    "        * currently have two open_orders.csv written to disk. need to figure out if both are being used\n",
    "        * in `/data/live/ports/<port_name>/open_orders.csv`\n",
    "        * and `/data/live/ports/<port_name>/orders/<exchange>/open_orders.csv`\n",
    "            * WE WANT THIS SECOND ONE ^^^^^^^^ \n",
    "        * see line ---- `state_dict['order_open_dict'] = {}`\n",
    "    * params --> state_dict migration \n",
    "    * `pair_info_df` should go from params --> state_dict \n",
    "        * allowing for client generation to be done much further down improving flow of things \n",
    "        * need an `update_pair_info_df()` if it will be stored in the `state_dict`\n",
    "    * on <b>docker</b>izing\n",
    "        * regarding open orders (see line ---- state_dict['order_open_dict'] = {}   contains all open orders, backed up live in  ./data/<port_name>/open_orders.csv \n",
    "            * figure out way to best handle that (ideally state_dict can be saved as a json, could also use a pickle)\n",
    "            * worst case read it from the file\n",
    "            * FOR NOW PICKLE `state_dict` EACH TIME... ITS THE FASTEST WAY TO GET LIVE then do JSON\n",
    "    * <b> ^^^^^^^^^^^^^ ALL THIS BEFORE DOCKERIZING ^^^^^^^^^^^^^ </b> \n",
    "    * loging in the live bot \n",
    "    * add print out of the decision state dict \n",
    "    \n",
    "* #### live plotting. \n",
    "    * either `live_plot_framework()`\n",
    "    * or include in existing one and add arg `mode='live'` or `'backtest'`\n",
    "\n",
    "* get teams notifications going \n",
    "\n",
    "* `one_run_params['decision']  \n",
    "    * this will be needed because the script `decision_on_signal.py` should be given a `decision_params` (or just a `one_run_params`?) \n",
    "\n",
    "* clickhouse trading summary\n",
    "    * in `push_trading_summary_to_clickhouse()` make sure that duplicate rows aren't being pushed (probably best to make a parameter `overwrite` defaulted to `True`) to \n",
    "\n",
    "* #### new data sources\n",
    "    * SP500 \n",
    "    * open source <b> bloomberg like </b> data source (there was some python thing)          \n",
    "* <b> FEATURE SET ADDITIONS </b>\n",
    "    * ratio of two EWM prices \n",
    "    * SP500, gold, broader commodities... \n",
    "    * <b> volume kernels </b> (5m, 1h, 4h, 1d, 1w, 1m) \n",
    "        * dont be concerned with efficiency... this will be difficult. Make it efficient after it works  \n",
    "        * kernels must be centered at the current price (otherwise it make no sense), SO..... \n",
    "            * to do this break the volume into price range bins that are much smaller than what then the final kernels to be used. Only significant issue will be on the shorter time scales if a mid price actually falls far from an interval. \n",
    "            * a way around this could be to give a \"metric\" for how far into the kernels above abnd below the current price we are (I don't like this though as there doesn't seem to be any value here having the data fed in non zero meaned then telling it where it is, just more complex space to learn. \n",
    "            * probably just do a grid +/-15% grid with 1% spacing \n",
    "\n",
    "* review how targets should be wrapped in a postprocessing function that directly outputs the product of `make_targets()` \n",
    "* ways to improve preformance in <b> rolling train </b>\n",
    "    * adaptive number of epochs based on loss\n",
    "    * roll in smaller steps, doing a triangled epoch setup (1 for 3 days ago, 2 for 2 days ago, 1 for 1 day ago, looping over a triangle, smallest last\n",
    "    * save loss of training in a few ways \n",
    "        * list of lists per rolling iter \n",
    "        * dataframe of ['start_date', 'end_date', 'start_loss', end_loss'] \n",
    "    * to handle changing regieme\n",
    "        * in rolling training keep going while the loss is going down by a certain ammount \n",
    "\n",
    "* investigate John's PNL function (should be in algos/decision.py) \n",
    "* clean up writing positions table \n",
    "* in decision algorithm, using `pred` instead of `signal` still \n",
    "* async stuff\n",
    "    * make `update_trading_summaries` async, this is the lowest running task thats in a docker container. \n",
    "    * then move up to `update_signal` \n",
    "* database flow improvement\n",
    "    * push [trades, trading_summary, signal] to clickhouse could all be put into one function. Most of it is the same\n",
    "        * same\n",
    "            * input: [exchange, symbol, overwrite_mode, existing_series, ]\n",
    "        *  different\n",
    "            * input [series_identifier] \n",
    "                * trades ---- trade_id... still select by a date range but then filter by trade id \n",
    "                * trading_summary ---- exchange, symbol, timestamp\n",
    "                * signal ---- signal_id, timestamp\n",
    "* docker ---> dont run as root "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f80c243-ac93-4f12-8bee-706957845c74",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div id='imports'> </div>\n",
    "\n",
    "# imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e68689-ec10-4d34-ae21-e2f5bdb83787",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d37e58",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %autoreload \n",
    "\n",
    "import dotenv\n",
    "# ### local imports ####PAUL TODO: clean up the sys path insertions \n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ###PAUL TODO: i feel like there maybe some runtime issues having params imported, not sure though \n",
    "from algos.config import params\n",
    "\n",
    "from algos.decision import (make_requests_dict,\n",
    "                            run_decision_algo,\n",
    "                            backtest_decision_making,\n",
    "                            get_port_value_and_return_series,\n",
    "                            get_sharpe_and_sortino,\n",
    "                            analyze_returns_at_start_and_end_of_threshold_break,\n",
    "                            get_feisable_transacts_list_from_time_strat,\n",
    "                            make_transacts_dict_from_list,\n",
    "                            run_simple_signal_threshold_and_hold_time_based_framework,)\n",
    "\n",
    "from algos.feature import ( print_feature_making_code,\n",
    "                            make_feature_df_from_results_of_printed_code,\n",
    "                            zip_hour_and_min_level_df, \n",
    "                            rolling_normalize_feature_df,get_multiasset_trading_summaries,\n",
    "                            make_utc_time_based_column,\n",
    "                            merge_dfs_with_index_granularity_matching,\n",
    "                            make_utc_time_df,\n",
    "                            make_eaors_trades_features, \n",
    "                            make_features, )\n",
    "\n",
    "from algos.feature_low_level import (get_ratio_of_metrics_rolling_sums,\n",
    "                                    # zscore,\n",
    "                                    calculate_momentum,\n",
    "                                    calc_macd,\n",
    "                                    calc_rsi,\n",
    "                                    calc_rolling_std,)\n",
    "\n",
    "from algos.gridsearch import (grid_search_over_parameters,\n",
    "                              make_results_df,\n",
    "                              run_full_process,\n",
    "                              analyze_framework_run,\n",
    "                              seperate_gridsearch_iter_dict_to_one_run_params, )\n",
    "from algos.model import (\n",
    "# rolling_decision_tree_train,\n",
    "                         make_mlp_dataloader, \n",
    "                         NeuralNetwork,\n",
    "                         train, \n",
    "                         test,\n",
    "                         make_predictions_for_dataloader,\n",
    "                         make_predictions_for_dataloader_features_only,\n",
    "                         reshape_preds,\n",
    "                         rolling_train_and_backtest_mlp,\n",
    "                         make_signal_dict_from_preds,)\n",
    "from algos.plotting import plot_framework\n",
    "from algos.targets import (make_targets,\n",
    "                           split_train_test,\n",
    "                           analyze_signal,\n",
    "                           print_signal_analysis, )  # ###PAUL this sort of thing should be housed under algos.utils....\n",
    "from algos.utils import (convert_date_format,\n",
    "                         get_date_list,\n",
    "                         convert_trades_df_to_trading_summary,\n",
    "                         find_runs,\n",
    "                         check_prices_for_gaps,\n",
    "                         get_trades_data, \n",
    "                         make_and_get_trading_summary,\n",
    "                         preprocess_data,\n",
    "                         preprocessing,  \n",
    "                         cut_two_pd_dti_objects_to_matching_dtis,\n",
    "                         fill_missing_minutes,\n",
    "                         fill_trading_summary_interpolating_missing_minutes,\n",
    "                         fill_trading_summary,\n",
    "                         check_minute_integrity,\n",
    "                         create_trading_summary_table, \n",
    "                         push_trading_summary_to_clickhouse,\n",
    "                         get_binance_data_zip_file,\n",
    "                         query_trading_summary,\n",
    "                         get_signal_id,\n",
    "                         deduplicate_df_on_index_only,\n",
    "                         get_secret,\n",
    ") \n",
    "\n",
    "\n",
    "# ###PAUL TODO: this should be depricated by code which prints a python file that does the calculations \n",
    "from algos.make_feature_set_printed_fn import make_feature_set_printed_fn \n",
    "# ###PAUL TODO: this should be depricated by code which prints a python file that does the calculations \n",
    "\n",
    "\n",
    "import ccxt\n",
    "from clickhouse_driver import Client as CH_Client\n",
    "from copy import deepcopy\n",
    "from collections import Counter\n",
    "import datetime\n",
    "from datetime import timedelta\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import itertools\n",
    "import json \n",
    "import lttb\n",
    "import math \n",
    "# import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import plotly\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "from scipy import signal\n",
    "import time\n",
    "import torch \n",
    "from torch import nn\n",
    "\n",
    "# THIS ISNT WORKING RIGHT NOW AND WE NEED IT TO BUT IGNORE FOR NOW \n",
    "# THIS ISNT WORKING RIGHT NOW AND WE NEED IT TO BUT IGNORE FOR NOW \n",
    "# THIS ISNT WORKING RIGHT NOW AND WE NEED IT TO BUT IGNORE FOR NOW \n",
    "# THIS ISNT WORKING RIGHT NOW AND WE NEED IT TO BUT IGNORE FOR NOW \n",
    "# THIS ISNT WORKING RIGHT NOW AND WE NEED IT TO BUT IGNORE FOR NOW \n",
    "# THIS ISNT WORKING RIGHT NOW AND WE NEED IT TO BUT IGNORE FOR NOW \n",
    "# THIS ISNT WORKING RIGHT NOW AND WE NEED IT TO BUT IGNORE FOR NOW \n",
    "\n",
    "\n",
    "# from torch.utils.data import DataLoaders\n",
    "\n",
    "ch_client = CH_Client(host=get_secret('CH_ALGOS_DB_HOST'),\n",
    "                      port=int(get_secret('CH_ALGOS_DB_PORT')),\n",
    "                      user=get_secret('CH_ALGOS_DB_USER'),\n",
    "                      password=get_secret('CH_ALGOS_DB_PASSWORD'), \n",
    "                      database=get_secret('CH_ALGOS_DB_DATABASE'))\n",
    "\n",
    "print(f\"done\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab69ca87-434a-4656-be9b-49d8b9a9bd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eecbb9f-edc4-42e1-987d-ec7fab561301",
   "metadata": {},
   "source": [
    "# make ccxt clients "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a2735e-9c90-4089-b639-40b94fc3365b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ccxt.pro import binanceus\n",
    "from ccxt import binanceus as binanceus_nonpro   # used for ad hoc shit, no async... \n",
    "\n",
    "# Connect to Binance\n",
    "binanceus = binanceus({\n",
    "    'apiKey': get_secret('BINANCE_DATA_1_PUBLIC'),\n",
    "    'secret': get_secret('BINANCE_DATA_1_PRIVATE'),\n",
    "})\n",
    "\n",
    "\n",
    "binanceus_nonpro = binanceus_nonpro({\n",
    "    'apiKey': get_secret('BINANCE_DATA_1_PUBLIC'),\n",
    "    'secret': get_secret('BINANCE_DATA_1_PRIVATE'),\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8750ecb1-7cee-425b-ac58-220181759f77",
   "metadata": {},
   "source": [
    "# non websocket method must use non pro "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d578c3e6-9cbc-4560-9aa8-240ccd1def6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch recent trades for a specific currency pair\n",
    "symbol = 'KDA/USDT'\n",
    "exchange = 'binance_us'\n",
    "trades = binanceus_nonpro.fetch_trades(symbol)\n",
    "\n",
    "# Print the trades\n",
    "# for i, trade in enumerate(trades):\n",
    "#     if \n",
    "#     print(trade)\n",
    "\n",
    "\n",
    "\n",
    "# import  pprint\n",
    "# pprint.pprint(trades[0]) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d881cd6-3ecc-4476-9702-bc3376f61ba3",
   "metadata": {},
   "source": [
    "# getting trades inserted into the database for a few different formats. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d01dfd-ce9c-44c7-9629-9388fbfa5fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pprint(trades)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f907e6-3d21-48dd-b4d3-bed1bd397627",
   "metadata": {},
   "outputs": [],
   "source": [
    "trade = trades[0]\n",
    "\n",
    "trade_0 = {'timestamp': datetime.datetime.utcfromtimestamp(trade['timestamp']/1000),  # ###PAUL TODO: gon need to verify this \n",
    "           'exchange': exchange,\n",
    "           'symbol': symbol,\n",
    "             'id': trade['id'], \n",
    "             'price': trade['price'],\n",
    "             'amount': trade['amount'],\n",
    "             'side': 1 if trade['side'] == 'buy' else 0 \n",
    "            }\n",
    "\n",
    "trade_1 = {'timestamp': datetime.datetime.utcfromtimestamp(trade['timestamp']/1000),  # ###PAUL TODO: gon need to verify this \n",
    "           'exchange': exchange,\n",
    "           'symbol': symbol,\n",
    "             'id': trade['id'], \n",
    "             'price': trade['price'],\n",
    "             'amount': trade['amount'],\n",
    "             'side': 1 if trade['side'] == 'buy' else 0 \n",
    "            }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5efaa48-bc58-40a2-9b87-3c9dbda37ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ch_client.execute('INSERT INTO algos_db.Trades VALUES ',\n",
    "                  [trade_0, trade_1,], \n",
    "                  # positions_df.reset_index().to_dict('records'),\n",
    "                  types_check=True,\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0218d4b-0d32-4383-9a2f-df542ddd0c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "ch_client.query_dataframe(\"\"\"SELECT *\n",
    "FROM algos_db.Trades\n",
    "ORDER BY timestamp DESC\n",
    "LIMIT 30;  -- Adjust the number as needed\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32ff2fc-d06f-4742-b0b6-1d2587c4b216",
   "metadata": {},
   "outputs": [],
   "source": [
    "trades_df = pd.DataFrame([trade_0, trade_1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723bd156-7b28-423d-b0fa-197ce1b10b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "trades_df.reset_index().to_dict('records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b794cfb-508a-418c-9fa2-2ac11d9da236",
   "metadata": {},
   "outputs": [],
   "source": [
    "trades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10f9c32-99b2-40c8-9960-a8bba059a9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "1701982151998"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f19e48-399c-498c-8a0e-ca65c6ac5895",
   "metadata": {},
   "outputs": [],
   "source": [
    "datetime.datetime.utcfromtimestamp(trades[0]['timestamp']/1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61943dab-bee2-4a2b-857b-d60218e10987",
   "metadata": {},
   "source": [
    "# websocket method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc8372e-87aa-4c85-bdb7-43da42a74bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76dd5586-e076-4639-b4b7-fdb33ef93be8",
   "metadata": {},
   "source": [
    "# websocket. one exchange multi symobol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17815020-1fbf-458f-b44a-9297568eebe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def process_trade_data(trades, ch_client, exchange):\n",
    "    \n",
    "    ch_formatted_trades = []\n",
    "    print(len(trades)) \n",
    "    for trade in trades: \n",
    "        # trade = {'timestamp': datetime.datetime.utcfromtimestamp(int(trade['timestamp']/1000)),  # ###PAUL TODO: gon need to verify this \n",
    "        trade = {'timestamp': datetime.datetime.utcfromtimestamp(trade['timestamp']/1000),  # ###PAUL TODO: gon need to verify this \n",
    "                 'exchange': exchange,\n",
    "                 'symbol': trade['symbol'],\n",
    "                 'id': trade['id'], \n",
    "                 'price': trade['price'],\n",
    "                 'amount': trade['amount'],\n",
    "                 'side': 1 if trade['side'] == 'buy' else 0 \n",
    "                }\n",
    "        ch_formatted_trades.append(trade)\n",
    "\n",
    "    ch_client.execute('INSERT INTO algos_db.Trades VALUES ',\n",
    "                      ch_formatted_trades, \n",
    "                      types_check=True,\n",
    "                     )\n",
    "    \n",
    "async def process_trades_periodically(processing_interval, ch_client):\n",
    "    while True:\n",
    "        if accumulated_trades:  # Check if there are trades to process\n",
    "            await process_trade_data(accumulated_trades, ch_client, exchange='binance')\n",
    "            accumulated_trades.clear()  # Clear the list after processing\n",
    "        await asyncio.sleep(processing_interval)\n",
    "\n",
    "# List to store accumulated trades\n",
    "accumulated_trades = []\n",
    "\n",
    "# Start processing task\n",
    "processing_interval = 10  # Process trades every 10 seconds\n",
    "processing_task = asyncio.create_task(process_trades_periodically(processing_interval, ch_client))\n",
    "\n",
    "# Main loop to collect trades indefinitely\n",
    "while True:\n",
    "    trades = await binanceus.watch_trades_for_symbols(symbols=['BTC/USDT', 'KDA/USDT', 'ETH/USDT', 'LINK/USDT', 'ROSE/USDT'] )\n",
    "    accumulated_trades.extend(trades)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e83752a-0a03-4d03-a946-b4b30ec866c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d431d5-57f2-440f-abe2-a89c2bb8fe2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from collections import defaultdict\n",
    "\n",
    "# Function to process trades for a specific exchange\n",
    "async def process_trade_data(trades, ch_client, exchange):\n",
    "    ch_formatted_trades = []\n",
    "    print(f\"trades collected for exchange: {exchange} ---> {len(trades)}\")\n",
    "    \n",
    "    for trade in trades:\n",
    "        formatted_trade = {\n",
    "            'timestamp': datetime.datetime.utcfromtimestamp(trade['timestamp']/1000),\n",
    "            'exchange': exchange,\n",
    "            'symbol': trade['symbol'],\n",
    "            'id': trade['id'], \n",
    "            'price': trade['price'],\n",
    "            'amount': trade['amount'],\n",
    "            'side': 1 if trade['side'] == 'buy' else 0\n",
    "        }\n",
    "        ch_formatted_trades.append(formatted_trade)\n",
    "    \n",
    "    ch_client.execute('INSERT INTO algos_db.Trades VALUES ', ch_formatted_trades, types_check=True)\n",
    "\n",
    "\n",
    "# Function to collect trades for each exchange\n",
    "async def collect_trades(exchange_client, exchange_name, symbols, trade_storage):\n",
    "    while True:\n",
    "        for symbol in symbols:\n",
    "            trades = await exchange_client.watch_trades_for_symbols(symbols=symbol)\n",
    "            trade_storage.extend([(exchange_name, trade) for trade in trades])\n",
    "        await asyncio.sleep(1)  # Adjust as needed\n",
    "\n",
    "# Process trades periodically\n",
    "async def process_trades_periodically(processing_interval, ch_client, trade_storage):\n",
    "    while True:\n",
    "        if trade_storage:  # Check if there are trades to process\n",
    "            for exchange, trade in trade_storage:\n",
    "                await process_trade_data(trade, ch_client, exchange)\n",
    "            trade_storage.clear()  # Clear the list after processing\n",
    "        await asyncio.sleep(processing_interval)\n",
    "\n",
    "# Exchange to symbols mapping\n",
    "exchange_symbols = {\n",
    "    'binance': ['BTC/USDT', 'KDA/USDT', 'ETH/USDT', 'LINK/USDT'],\n",
    "    # Add other exchanges and their symbols as needed\n",
    "}\n",
    "\n",
    "# List to store accumulated trades\n",
    "accumulated_trades = []\n",
    "\n",
    "# Start trade collection tasks for each exchange\n",
    "collection_tasks = []\n",
    "for exchange, symbols in exchange_symbols.items():\n",
    "    # Create an exchange client instance for each exchange\n",
    "    exchange_client = create_exchange_client(exchange)  # Replace with actual client creation logic\n",
    "    task = asyncio.create_task(collect_trades(exchange_client, exchange, symbols, accumulated_trades))\n",
    "    collection_tasks.append(task)\n",
    "\n",
    "# Start processing task\n",
    "processing_interval = 10  # Process trades every 10 seconds\n",
    "processing_task = asyncio.create_task(process_trades_periodically(processing_interval, ch_client, accumulated_trades))\n",
    "\n",
    "# Run the event loop\n",
    "loop = asyncio.get_event_loop()\n",
    "loop.run_until_complete(asyncio.gather(*collection_tasks, processing_task))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83250e2-27c6-4b90-b1bf-635a6c70597c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "92af5092-7b72-4b0c-91bd-19c6ac585002",
   "metadata": {},
   "source": [
    "# ### ATTEMPTING TO GET MULTIPL3E EXCHANGES AND TICKERS WORKING IN ONE COLLECTION SCRIPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d907a9e-21b7-45c4-8ea7-b0615c7a0c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from collections import defaultdict\n",
    "\n",
    "async def collect_trades(exchange, symbol, trade_storage):\n",
    "    while True:\n",
    "        trades = await exchange.watch_trades(symbol)\n",
    "        trade_storage[exchange.name][symbol].extend(trades)\n",
    "        await asyncio.sleep(1)  # Adjust frequency as needed\n",
    "\n",
    "async def process_trade_data(trades, ch_client):\n",
    "    # Your existing processing logic here\n",
    "    ...\n",
    "\n",
    "async def process_trades_periodically(processing_interval, trade_storage, ch_client):\n",
    "    while True:\n",
    "        for exchange_name in trade_storage:\n",
    "            for symbol in trade_storage[exchange_name]:\n",
    "                if trade_storage[exchange_name][symbol]:\n",
    "                    await process_trade_data(trade_storage[exchange_name][symbol], ch_client)\n",
    "                    trade_storage[exchange_name][symbol].clear()\n",
    "        await asyncio.sleep(processing_interval)\n",
    "\n",
    "# Dictionary to store accumulated trades, organized by exchange and ticker\n",
    "accumulated_trades = defaultdict(lambda: defaultdict(list))\n",
    "\n",
    "# List of exchanges and their symbols\n",
    "exchanges = [binanceus,] #  another_exchange]  # Replace with actual exchange objects\n",
    "symbols = ['BTC/USDT', 'ETH/USDT']  # Replace with the list of tickers you're interested in\n",
    "\n",
    "# Start trade collection tasks\n",
    "collection_tasks = [asyncio.create_task(collect_trades(exchange, symbol, accumulated_trades)) \n",
    "                    for exchange in exchanges for symbol in symbols]\n",
    "\n",
    "# Start processing task\n",
    "processing_interval = 10  # Process trades every 10 seconds\n",
    "processing_task = asyncio.create_task(process_trades_periodically(processing_interval, accumulated_trades, ch_client))\n",
    "\n",
    "# Run the event loop\n",
    "loop = asyncio.get_event_loop()\n",
    "loop.run_until_complete(asyncio.gather(*collection_tasks, processing_task))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19dc4076-4878-4d1e-91c3-5aae1159a19b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7bb163a-4d0c-48cb-824b-efb7af1801de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1887638-dbd3-4da7-8d43-e1d8d0837290",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79091d02-0ab0-4b27-b7a5-961990dd9391",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0a5f21-9673-40c7-8152-cf322a5f8889",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c617fa8-842c-4138-9fcc-5bc2bf1939ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "33d947e3-2aed-45ed-ba42-71853b192ebf",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "[back to top](#outline_algos_framework) \n",
    "\n",
    "<div id='setup'> </div>\n",
    "\n",
    "# setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3d46d0-3bb8-4cae-8348-3d62e7970c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install ccxt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c740c2b2-a913-43a1-8e68-dd69143fe193",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div id='HANDLING__SIGNAL_DICT__MODEL__FRAMEWORK_RESULTS__PIPELINE'> </div> \n",
    "\n",
    "* ##### save and load key objects\n",
    "    * [`target_series`](#loading_target_series)\n",
    "    * [`signal_dict`](#loading_signal_dict)\n",
    "    * [`framework_results`](#loading_framework_results)\n",
    "    * [`gs_results`](#loading_gs_results)\n",
    "    * [`one_run_params`](#loading_one_run_params)\n",
    "    \n",
    "    * ##### ###PAUL TODO: this should probably go in utils\n",
    "        * but due to the recent need to reorganize keep in NB and interate untill confident that the `signal_dict`/`model` --> `framework_results` pipeline won't need significant adjustment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58ee6c4-b2d0-4b10-8700-8de96401c25c",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div id='loading_target_series'> </div>\n",
    "\n",
    "* ### save and load `target_series`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b0f1d8-5e2b-478d-99fd-c4f9734f060d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# del signal_dict_fp\n",
    "\n",
    "# # ### NAME OF `signal_dict` run\n",
    "# #\n",
    "# #\n",
    "target_series_name = f\"target_serires____2023_04_25____peaks_0_06____min_peak_dist_480____interval_len_14d____smooth_0_01\"\n",
    "target_series_fp = f\"{params['dirs']['data_dir']}pickled_target_series/{target_series_name}.pickle\"\n",
    "\n",
    "# # ### SAVING\n",
    "# #\n",
    "# pickle.dump(target_series,  #   <<<------- COPY PASTE DESIRED VARIABLE HERE\n",
    "#             open(target_series_fp, \"wb\"))\n",
    "\n",
    "# # ### OPENING\n",
    "# #\n",
    "# with open(target_series_fp, 'rb') as f:\n",
    "#      target_series = pickle.load(f) #   <<<------- COPY PASTE DESIRED VARIABLE HERE\n",
    "\n",
    "# y = target_series # name y for work down low # ###PAUL TODO: variable name conflict\n",
    "\n",
    "### LEAVE ON ALWAYS\n",
    "#\n",
    "del target_series_fp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09dc420-a8c9-4fd9-bdeb-34ea131d8c80",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div id='loading_signal_dict'> </div>\n",
    "\n",
    "* ### save and load `signal_dict`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6dda04-6483-46f0-89ba-a00f13067157",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# # ### NAME OF `signal_dict` run\n",
    "# #\n",
    "# #\n",
    "# signal_dict_name = f\"signal_dict____2023_08_01____mlp_rolling____first_multi_asset_feature_set\"  \n",
    "# signal_dict_name = f\"signal_dict____2023_08_03___mlp_rolling____validate\"  \n",
    "# signal_dict_name = f\"signal_dict____2023_08_23___mlp_rolling____to_2023_07_18\"\n",
    "# signal_dict_name = f\"signal_dict____2023_09_01___mlp_rolling_smaller_model\"\n",
    "signal_dict_name = f\"signal_dict____2023_09_01___mlp_rolling____large_model\"\n",
    "# signal_dict_name = f\"signal_dict____2023_09_01___mlp_rolling____large_model_bigger_learning_rate\"\n",
    "# signal_dict_name = f\"signal_dict____2023_09_01___mlp_rolling____large_model_SGD_1e4\"\n",
    "\n",
    "signal_dict_fp = f\"{params['dirs']['data_dir']}pickled_signal_dicts/{signal_dict_name}.pickle\"\n",
    "\n",
    "# # ### SAVING\n",
    "# #\n",
    "# if os.path.isfile(signal_dict_fp): \n",
    "#     print(f\"signal already exists at that filepath\") \n",
    "# else:\n",
    "#     pickle.dump(signal_dict,  #   <<<------- COPY PASTE DESIRED VARIABLE HERE\n",
    "#                 open(signal_dict_fp, \"wb\"))\n",
    "\n",
    "\n",
    "# # ### OPENING\n",
    "# #\n",
    "with open(signal_dict_fp, 'rb') as f:\n",
    "     signal_dict = pickle.load(f) #   <<<------- COPY PASTE DESIRED VARIABLE HERE\n",
    "\n",
    "### LEAVE ON ALWAYS\n",
    "#\n",
    "del signal_dict_name; del signal_dict_fp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8cf6c1-b154-4940-9713-e294fb0e3268",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "signal_dict.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01bd245-083e-4611-a638-0c2a47fa00bb",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div id='loading_framework_results'> </div>\n",
    "\n",
    "* ### save and load `framework_results`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e340fd3-9fa3-4bdd-86e7-7de0cc0f8059",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# # ### NAME OF `framework_results` \n",
    "#\n",
    "framework_results_name = f\"framework_results____0_06_peaks____MSE_loss____on_preds__0_0075_ewm__4_day_rolling_norm____split_0_65__0_15____NO_shuffled_train\"\n",
    "framework_results_fp = f\"{params['dirs']['data_dir']}pickled_signal_dicts/{framework_results_name}.pickle\"\n",
    "\n",
    "# # ### SAVING\n",
    "# #\n",
    "# if os.path.isfile(framework_results_fp): \n",
    "#     print(f\"FRAMEWORK RESULTS already exists at that filepath\")\n",
    "# else:\n",
    "#     pickle.dump(framework_results,  #   <<<------- COPY PASTE DESIRED VARIABLE HERE\n",
    "#                 open(framework_results_fp, \"wb\"))\n",
    "\n",
    "# # ### OPENING\n",
    "# #\n",
    "# with open(framework_results_fp, 'rb') as f:\n",
    "#      framework_results = pickle.load(f) #   <<<------- COPY PASTE DESIRED VARIABLE HERE\n",
    "        \n",
    "### LEAVE ON ALWAYS\n",
    "#\n",
    "del framework_results_name; del framework_results_fp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2deee6e7-c836-4e03-b9ff-7419b0869eb3",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div id='loading_gs_results'> </div>\n",
    "\n",
    "* ### save and load `gs_results`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceee61db-2995-424b-8299-3a5e4fbe2d7e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# ### reading in prior grid searches\n",
    "#\n",
    "\n",
    "gs_file = f\"grid_search____2023_08_01____mlp_rolling____first_multi_asset_feature_set\"\n",
    "\n",
    "gs_file = f\"grid_search____2023_08_01____mlp_rolling____v2\"  # 1\n",
    "# gs_file = f\"grid_search____2023_08_01____mlp_rolling____v3\"  # 2\n",
    "# gs_file = f\"grid_search____2023_08_01____mlp_rolling____v4\"    # 1\n",
    "# gs_file = f\"grid_search____2023_08_29____mlp_rolling_v2\"\n",
    "# gs_file = f\"grid_search____2023_09_05____on____signal_dict____2023_09_01___mlp_rolling_smaller_model\"\n",
    "# gs_file = f\"grid_search____2023_09_06____on____signal_dict____2023_09_01___mlp_rolling_smaller_model____any_two_set\"\n",
    "\n",
    "# gs_file = f\"grid_search____most_recent.pickle\" \n",
    "\n",
    "gs_fp = f\"{params['dirs']['data_dir']}gridsearches/{gs_file}.pickle\"\n",
    "\n",
    "# # ### SAVING gridsearch\n",
    "# # \n",
    "# if os.path.isfile(gs_fp): \n",
    "#     print(f\"gs_df already exists at that filepath\")\n",
    "# else:\n",
    "#     gs_df.to_pickle(gs_fp)\n",
    "\n",
    "# # ### OPENING gridsearch \n",
    "# # \n",
    "gs_df = pd.read_pickle(gs_fp)\n",
    "\n",
    "### LEAVE ON ALWAYS\n",
    "#\n",
    "del gs_file; del gs_fp\n",
    "\n",
    "\n",
    "#\n",
    "# ###\n",
    "# ### getting columns we are interested in seeing the results of \n",
    "# ###\n",
    "#\n",
    "col_subset = []\n",
    "for col in gs_df.columns:\n",
    "    if 'decision_' in col: \n",
    "        col_subset.append(col) \n",
    "preformance_cols = ['sharpe', 'sortino', 'pnl', 'num_transacts', 'ranking_metric', ]\n",
    "show_cols = col_subset + preformance_cols\n",
    "\n",
    "\n",
    "gs_df['ranking_metric'] = gs_df['sharpe']**3 * gs_df['pnl'] / gs_df['num_transacts' ]**(2.5) *1_000_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f010eea-1be4-4c15-b26c-a896caaa4dde",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "gs_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7993f2a3-7be2-42e4-9bd5-deb9ff1496f5",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799e5de4-9599-4a77-8e93-9e357063d18d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "gs_df.sort_values(by='pnl').iloc[-10:][show_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dfb0fa3-794b-4ba8-b188-78ab865b6429",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1e855d-03c4-4d64-a803-8acc3125b67a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efafea6f-8b26-4940-ad22-9a2f2ee29c1b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33184fee-3853-4855-9033-220fb3b62837",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "gs_df.sort_values(by='pnl').iloc[-10:][show_cols]\n",
    "\n",
    "\n",
    "# gs_df.sort_values(by='pnl').iloc[-10:][show_cols]\n",
    "# gs_df.sort_values(by='ranking_metric')[show_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7b3080-5065-4a86-89cf-5eb9ba511f56",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# decision_threshold\tdecision_pred_dist\tdecision_price_dist\tdecision_stop_limit\t\tdecision_to_neutral_threshold\tdecision_to_neutral_pred_dist\tdecision_to_neutral_price_dist\tdecision_to_neutral_stop_limit\n",
    "np.sort(gs_df['decision_to_neutral_stop_limit'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e320c8-f61a-4d6a-9090-0b82894447b8",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "* ##### <i> exploring `gs_results`  </i> ---- isolating observations with by a column pandas can't match by comparing equality via `==`  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560e5301-e511-44bc-b762-0ff4bef04057",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# cmp = lambda x: x==[]\n",
    "cmp = lambda x: x==['threshold', 'pred_dist', 'price_dist']\n",
    "mask_decision_to_neutral_any_two = gs_df['decision_to_neutral_any_two'].apply(cmp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ea4107-ba5f-4a8c-a140-06333b476b30",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "* ##### `one_run_params` derived from `gs_df`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081583dc-0361-447c-a4cc-d51eaf2b1bbf",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# ### getting one run params\n",
    "#\n",
    "#\n",
    "gs_df['ranking_metric'] = gs_df['sharpe'] * gs_df['pnl'] / gs_df['num_transacts' ] \n",
    "\n",
    "one_run_params = seperate_gridsearch_iter_dict_to_one_run_params(gs_df.sort_values(by='ranking_metric').iloc[-1])\n",
    "avg_best = gs_df.sort_values(by='ranking_metric').tail(100).mean()\n",
    "\n",
    "for name, val in avg_best.iteritems():\n",
    "    for param_type in ['feature_', 'targets_', 'model_', 'decision_', ]:\n",
    "        param_no_underscore = param_type.replace(\"_\", \"\") \n",
    "        if name.startswith(param_type):\n",
    "            name = name.replace(param_type, \"\")\n",
    "            # print(f\"{type(one_run_params[param_no_underscore][name])}\")\n",
    "            if type(one_run_params[param_no_underscore][name]) in [np.float64, float]: \n",
    "                # print(f\"name: {name} ---- param_type: {param_type} ---- param_no_type_prefix: {param_no_underscore}\")\n",
    "                one_run_params[param_no_underscore][name] = val\n",
    "                \n",
    "# # ### GET THE PARAMS FROM ONE INDIVIDUAL RUN ----  EX: just want to run the best model on a grid search \n",
    "# #\n",
    "# #\n",
    "one_run_params = seperate_gridsearch_iter_dict_to_one_run_params(gs_df.sort_values(by='pnl').iloc[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f656af5c-e63c-4473-8eda-7a5e18b02156",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "[back to top](#outline_algos_framework) \n",
    "\n",
    "\n",
    "<div id='loading_one_run_params'> </div>\n",
    "\n",
    "* ### save and load `one_run_params` \n",
    "    * # ###PAUL TODO: for now hard coding `desired_features_dict` but this is where I would like this to go for the flow of the notebook "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d7db5f-e41a-4ff0-8857-f690f5845724",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# ### dict containing atleast one of every option\n",
    "#\n",
    "#\n",
    "desired_features_dict = {\"buy_to_sell_count_ratio\": {\"interval_lens\": [1, 3, 10, 28, 59, 119, 360],\n",
    "                                                     \"function_name\": \"get_ratio_of_metrics_rolling_sums\",\n",
    "                                                     # note, if the argument is str it is written \"'string'\"\n",
    "                                                     \"kwargs\": {\"df\": \"trading_summary\",\n",
    "                                                                \"metric_1\": \"'buyer_is_maker'\",\n",
    "                                                                \"metric_2\": \"'buyer_is_taker'\",\n",
    "                                                                }\n",
    "                                                     },\n",
    "\n",
    "                         \"buy_to_sell_vol_ratio\": {\"interval_lens\": [1, 3, 10, 28, 59, 119, 360, 1440, ],\n",
    "                                                   \"function_name\": \"get_ratio_of_metrics_rolling_sums\",\n",
    "                                                   \"kwargs\": {\"df\": \"trading_summary\",\n",
    "                                                              \"metric_1\": \"'buy_base_vol'\",\n",
    "                                                              \"metric_2\": \"'sell_base_vol'\",\n",
    "                                                              },\n",
    "                                                   },\n",
    "\n",
    "                         \"momentum\": {\"interval_lens\": [1, 3, 10, 28, 59, 119, 240, 1200, 4320, 14400],\n",
    "                                      \"function_name\": \"calculate_momentum\",\n",
    "                                      \"kwargs\": {\"df\": \"trading_summary\",\n",
    "                                                 \"col\": \"'vwap'\",\n",
    "                                                 },\n",
    "                                      },\n",
    "\n",
    "                         \"rsi\": {'interval_lens': [10, 28, 59, 240, 1200, 4320, 14400, 56000],\n",
    "                                 'function_name': 'calc_rsi',\n",
    "                                 'kwargs': {'series': \"trading_summary['vwap']\"},\n",
    "                                 },\n",
    "\n",
    "                         # ###PAUL this is not what is wanted... rolling variance is a good replacement for what this was trying to do \n",
    "                         # \"roll_std_price\": {'interval_lens': [15, 28, 59, 119, 240, 1200, 4320, 14400],\n",
    "                         #                    'function_name': 'calc_rolling_std',\n",
    "                         #                    'kwargs': {'series': \"prices['vwap']\"},\n",
    "                                            # },\n",
    "                         \"roll_std_vol\": {'interval_lens': [15, 28, 59, 119, 240, 1200, 4320, 14400],\n",
    "                                          'function_name': 'calc_rolling_std',\n",
    "                                          'kwargs': {'series': \"prices['total_base_vol']\"},\n",
    "                                          },\n",
    "                         \n",
    "                         \n",
    "                         # ###PAUL TODO: ROLLING STD OF VOLATILITY \n",
    "                         #\n",
    "                         #\n",
    "\n",
    "                         # more complex metrics below this line\n",
    "                         # _=_=_=_=_=_=_=_=_=_=_=_=_=_=_=_=_=_=_=_=_=_=_=_=_=_=_=_=_=_=_=_=_=_=_=_=_\n",
    "\n",
    "                         'macd': {'function_name': 'calc_macd',\n",
    "                                  'kwargs': {'series': \"prices['vwap']\",\n",
    "                                             },\n",
    "                                  'span_tuples': [(12, 26, 9), (4, 26, 9), (13, 26, 1), (12, 26, 3)],\n",
    "                                  'multipliers': [1, 5, 15, 60, 150, 1600 ],\n",
    "                                  },\n",
    "\n",
    "                         # EXAMPLE: keep this feature_name it is skipped by the code writer if left as is\n",
    "                         # _=_=_=_=_=_=_=_=_=_=_=_=_=_=_=_=_=_=_=_=_=_=_=_=_=_=_=_=_=_=_=_=_=_=_=_=_\n",
    "                         'feature_name': {'interval_lens': [30, 60, 120, ],\n",
    "                                          'function_name': 'function_to_make_metric',\n",
    "                                          'kwargs': {'col_name': 'input_str',\n",
    "                                                     'metric_1': 'input_str',\n",
    "                                                     'metric_2': 'input_str',\n",
    "                                                     }\n",
    "                                          },\n",
    "                         }\n",
    "\n",
    "\n",
    "# print_feature_making_code(desired_features_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf8c08b-a220-4a15-b634-2a21605f31ab",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "feature_params = {\n",
    "    # for live functionality what process historical EAORS trades sits between `update_trading_summary.py` \n",
    "    \n",
    "    # ###PAUL TODO: consider refactoring sources to feature_params['sources'][source] \n",
    "    # 'sources': {                   dictionary continued below, but not used for now \n",
    "    \n",
    "    \n",
    "    # TODO: `eaors_trades` relies on the same pair only appearing once, this is unrealistic as we may want the same pair from two exchanges.\n",
    "    # seems like the best approach would be [exchange][pair] ... with the note that this would not support spot vs futures. \n",
    "    # to future proof, it would be necessary to use ['eaors_trades']['spot/futures'][exchange][pair] then walk down that tree starting at `eaors_trades`\n",
    "    \n",
    "    \n",
    "    # TODO: `max_data_mode` ---- have function get all inputs from as early as possible \n",
    "    # TODO: also provide a dictionary of how early each source was, the cut (total or broken down?) needed to process it and limiting feature\n",
    "    'eaors_trades': { # 'data_source' : {source_specific_processing_information}, \n",
    "                      # ###PAUL TODO: with this structure there is no way to do cross exchange metrics, which could offer significant insight\n",
    "                      # ###PAUL TODO: for example... binance_volume / kucoin_volume along with price....\n",
    "                      # ###PAUL TODO:    - volume would have to be ratio of rolling sum, price could be the instaneous (\n",
    "                      # ###PAUL TODO: regardless, this would be something that could be interesting \n",
    "                      # ###PAUL TODO: \n",
    "                      'BTC-USDT': {'desired_features_dict': desired_features_dict,\n",
    "                                  'exchange': 'binance',\n",
    "                                  'start_date': (2018, 1, 1), \n",
    "                                  'end_date': (2023, 7, 18), \n",
    "                                  'alternative_data_pair': None,\n",
    "                                 },\n",
    "                     'BTC-TUSD': {'desired_features_dict': desired_features_dict,\n",
    "                                  'exchange': 'binance',\n",
    "                                  'start_date': (2018, 1, 1), \n",
    "                                  'end_date': (2023, 7, 18), \n",
    "                                  'alternative_data_pair': 'BTC-USDT',\n",
    "                                  'alternative_data_exchange': 'binance',\n",
    "                                  'alternative_start_date': (2018, 1, 1), \n",
    "                                  'alternative_end_date': (2023, 2, 15), \n",
    "                                 },\n",
    "                     'ETH-USDT': {'desired_features_dict': desired_features_dict,\n",
    "                                  'exchange': 'binance',\n",
    "                                  'start_date': (2018, 1, 1), \n",
    "                                  'end_date': (2023, 7, 18), \n",
    "                                  'alternative_data_pair': None,\n",
    "                                 },\n",
    "                     'ETH-TUSD': {'desired_features_dict': desired_features_dict,\n",
    "                                  'exchange': 'binance',\n",
    "                                  'start_date': (2018, 1, 1), \n",
    "                                  'end_date': (2023, 7, 18), \n",
    "                                  'alternative_data_pair': None,\n",
    "                                  'alternative_data_exchange': 'binance',\n",
    "                                  'alternative_start_date': (2018, 1, 1), \n",
    "                                  'alternative_end_date': (2023, 2, 15), \n",
    "                                 },                        \n",
    "    },\n",
    "    \n",
    "    # 'eaors_orderbook': {}, \n",
    "    # 'l_map': {}, \n",
    "    # 'sp500': {},\n",
    "    # 'commodities': {},\n",
    "    # 'forex': {'DXY': {'desired_features_dict': desired_features_dict},  # could be nearly identical to EAORS trade pipeline \n",
    "    \n",
    "    'utc_time': True,  \n",
    "    \n",
    "    # ###PAUL TODO: consider refactoring sources to feature_params['sources'][source] \n",
    "    #\n",
    "    # }                  dictionary started above, but not used for now \n",
    "        \n",
    "    'preprocess_rolling_norm_n_obvs' : 20 * 24 * 60,\n",
    "    \n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "target_params= \\\n",
    "    {\n",
    "     'min_peak_dist': 480,\n",
    "     'peak_prominence': 0.055,\n",
    "     'interval_len_in_steps': 35*24*60,\n",
    "     'pd_ewm_alpha': 0.01\n",
    "    }\n",
    "\n",
    "model_params = \\\n",
    "    {\n",
    "        'train_days': 365,\n",
    "        'step_size': 7,\n",
    "        'f_window': 0,\n",
    "\n",
    "        'num_epochs_for_train': 14,\n",
    "        'num_epochs_when_rolling': 3,\n",
    "        \n",
    "        'batch_size': 24 * 60,\n",
    "\n",
    "        # dataloader shuffle stuff (planned for batch training, but may want to add to rolling) \n",
    "        'shuffle_train': False,\n",
    "        'shuffle_test': False,  # this should always be False, maybe dont give it an argument\n",
    "\n",
    "        'volatility_normalization_for_targets': False,  # ###PAUL TODO: check this calculation, OFF for now\n",
    "        'return_window_n': 3*60,         # REQUIRED IF `volatility_normalization_for_targets` == True\n",
    "        'variance_window_n': 3*24*60,    # REQUIRED IF `volatility_normalization_for_targets` == True\n",
    "\n",
    "        'normalize_targets': True,  # normalizes targets per each iteration BEFORE cutting them for training\n",
    "        # 'optimizer': torch.optim.Adam(model.parameters(), lr=1e-3),  \n",
    "        # MAYBE NEED TO ADD optimizer TO BE DONE AFTER MODEL CREATION.... \n",
    "        # WITH THAT BEING SAID IIT MAY MAKE SENSE TO ADD MODEL INITIALIZATION IN THESE PARAMS \n",
    "        # ^^^^^^^^^^^\n",
    "    \n",
    "        'loss_fn': nn.MSELoss(),\n",
    "        \n",
    "        # ### preds post processing parameters to make preds into a signal \n",
    "        'preds_pd_ewm_alpha': 0.01,\n",
    "        'rolling_normalize_preds': True,\n",
    "        'signal_norm_window': 5*24*60,   # REQUIRED IF `rolling_normalize_preds` == True\n",
    "    }\n",
    "\n",
    "decision_params = \\\n",
    "    {\n",
    "        \n",
    "    } \n",
    "\n",
    "one_run_params = \\\n",
    "    {'feature': feature_params,\n",
    "      'target': target_params,\n",
    "      'model': model_params,\n",
    "      'decision': decision_params,\n",
    "     }\n",
    "\n",
    "# this belongs in params (i believe parts gets put in it, but after going multi asset it would be a good thing to try and do as \n",
    "# as close of a merger as possible between the notebook method and scripts building everything off of configs.py regardless) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "650159e8-9b25-4f58-93eb-80d9c827fb26",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div id='run_parameters'> </div>\n",
    "\n",
    "## run parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77e78d8-aa85-4250-8a74-9407441dc46d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# ### notebook control variables\n",
    "#\n",
    "#\n",
    "nb_params = \\\n",
    "{\n",
    "    'read_trading_summary_from_db': True,\n",
    "    \n",
    "    # ###PAUL TODO: should be moved to `one_run_params['features']\n",
    "    'make_min_features': True,\n",
    "    'make_cq_features': False,\n",
    "        \n",
    "    'make_example_targets': False,\n",
    "    'discresionary_signal': False,\n",
    "    'single_run': True, \n",
    "    'gridsearch_run': False,\n",
    "    'one_full_run': False,\n",
    "    'live_trade_work': False, \n",
    "    'neural_net_dev': True, \n",
    "    'MLP_batch': False,\n",
    "    'MLP_rolling': True,  \n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "802fe174-fcb5-4a4d-872f-ed67aa5b433f",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div id='target_and_feature_formation'> </div>\n",
    "\n",
    "## feature and target <b> formation </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547a9ab6-e068-4211-bff5-09ff2269fb35",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "* #### CREATE `trading_summary` from quering trades and summarizing\n",
    "    * note `trading_summary` migrated to clickhouse which is now upated live but kept for example to help with understanding the pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4889b3a2-0a2d-4205-977c-fb1e5b2cebc4",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "del run_once_make_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1150923e-8c91-459e-be78-a34112ec4980",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "try:\n",
    "    if run_once_make_features: \n",
    "        raise ValueError\n",
    "except NameError:\n",
    "    run_once_make_features = True \n",
    "    \n",
    "x, max_cut_time = make_features(feature_params=feature_params, ch_client=ch_client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf8fd88-6ef0-4217-a317-e3b5dad23cad",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507b0973-a0b3-4a72-a23d-684436be8ec1",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "x.index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2126e87c-af34-4bd5-bd0d-773e1d582df9",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "* ### ###PAUL TODO: `vwap_dict` is needed for \n",
    "    * target (live and backtesting)... though I think we can use trading_summaries. look into this \n",
    "    * backtesting\n",
    "    * plotting \n",
    "    * ##### figure out the best place to put it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3cff031-0959-49ad-8b6a-4e93bb3e8807",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "vwap_dict = get_multiasset_trading_summaries(feature_params['eaors_trades'], columns=['vwap'], ch_client=ch_client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e4c31d-9973-433d-be10-ef2d84fe5c8b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "vwap_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a96dfac-0dbf-4651-976b-86d200c60fdb",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "* ##### crypto quant data processing (currently depricated ---- keeping around because potential for revival) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace15e06-88d6-4706-bedf-f56bf2170ce3",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # ### CRYPTO QUIANT PROCESSING KEPT INCASE WE WANTED TO BRING THE DATA IN IN THE FUTURE (OR TO SEE IF IT HELPS) \n",
    "# #\n",
    "# #\n",
    "# if nb_params['make_cq_features']: \n",
    "#     crypto_quant_freq = \"hour\"\n",
    "#     start_date = \"2016-12-31\"  # decent date to start general prune as many features NaN before this point \n",
    "#     ticker = \"btc\"\n",
    "#     pct_nan = 0.05\n",
    "\n",
    "#     # read in signal and info_df which contains metadata for columns in signal (api link, min / max date) \n",
    "#     signal_fp = f\"{params['dirs']['data_dir']}crypto_quant/btc/signal_hour.pickle\"\n",
    "    \n",
    "#     with open(signal_fp, 'rb') as f:\n",
    "#         file = pickle.load(f)\n",
    "#         hour_crypto_quant = file[ticker][0]\n",
    "#         df_info = file[ticker][1]\n",
    "#         hour_crypto_quant.index = hour_crypto_quant['date']  ###PAUL new add del_later\n",
    "#         hour_crypto_quant.index = pd.to_datetime(hour_crypto_quant.index)\n",
    "#         hour_crypto_quant.index.name = 'Timestamp'  # all pd.DTI's are named Timestamp\n",
    "\n",
    "#         del hour_crypto_quant['date']\n",
    "#         hour_crypto_quant.sort_values(by='Timestamp', inplace=True)\n",
    "\n",
    "#     hour_crypto_quant = hour_crypto_quant[hour_crypto_quant.index >= start_date]\n",
    "#     hour_crypto_quant_return = preprocessing(hour_crypto_quant, pct_nan, \"return\")\n",
    "#     hour_crypto_quant_z_score = preprocessing(hour_crypto_quant, pct_nan, \"z_score\")\n",
    "\n",
    "#     # add column name identifies for merge (makes it work easier too) \n",
    "#     hour_crypto_quant_return.columns = ['cq_return_' + col for col in hour_crypto_quant_return.columns]\n",
    "#     hour_crypto_quant_z_score.columns = ['cq_zscore_' + col for col in hour_crypto_quant_z_score.columns]\n",
    "\n",
    "#     hourly_feature_df = hour_crypto_quant_return.merge(right=hour_crypto_quant_z_score,\n",
    "#                                                        how='outer',\n",
    "#                                                        left_index=True,\n",
    "#                                                        right_index=True,\n",
    "#                                                       )\n",
    "\n",
    "#     hourly_feature_df.shape\n",
    "    \n",
    "# print(f\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97af63a3-882c-4207-a57c-7f5fdb889977",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "* #### find gaps in `prices` which represents gaps in trade level data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6353f8-6a6a-4f3c-a46b-187df20d767e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "info_on_missing_price_data = check_prices_for_gaps(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b022224f-40e9-40fe-8764-47a527edf94e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "x.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e279e48-80ba-4e26-9f2e-ca83831c1ebe",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "x.tail(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3797865b-e826-4138-8a6b-e52fc3b2729f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# ### ensure that prices contain every index within the features set \n",
    "# ### ALWAYS SHOULD RUN THIS CHECK \n",
    "#\n",
    "#\n",
    "trading_summary_range_of_features_mask = np.logical_and(min(x.index) <= trading_summary.index,  trading_summary.index <= max(x.index))\n",
    "pd.testing.assert_index_equal(trading_summary[trading_summary_range_of_features_mask].index, x.index)\n",
    "assert trading_summary[trading_summary_range_of_features_mask].index.equals(x.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f11b3e63-ffc5-4551-84db-921c2a2df06e",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div id='example_target_production'> </div>\n",
    "\n",
    "## example target production \n",
    "* due to the nature of the targets they need to be calculated at the time of training\n",
    "* this is to explore how changes to target generation affect a good estimate of a real time implementable target series "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f02308b-5de4-401a-babb-0f1f13b296d9",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "vwap_dict['BTC-TUSD']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856a3297-83b8-45a7-a782-0c3e1cac1776",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "nb_params['make_example_targets'] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b1ec52-4245-48bd-8e15-6517addb03e5",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# nb_params['make_example_targets'] = True\n",
    "if nb_params['make_example_targets'] == True:  # ###PAUL neural_net_dev shoulnd be removed when no longer needed  \n",
    "\n",
    "    y, ideal_buy_sell_dict = make_targets(series=vwap_dict['BTC-TUSD']['vwap'], **one_run_params['target'] ) \n",
    "          \n",
    "print(f\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7d019f-1c22-4dc1-b4d5-25bd89ecb2ac",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eaac2bf-a82a-4db0-abbb-e5b7d1374415",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if nb_params['make_example_targets'] == True:\n",
    "    \n",
    "    target_results = {'preds': y}   \n",
    "    \n",
    "    # ### PLOTTING SIGNAL\n",
    "    #\n",
    "    plot_requests = {\n",
    "        'start_date': (2021, 9, 10),  # ###PAUL TODO: add start / end functionality to here \n",
    "        'end_date': (2021, 12, 10),\n",
    "        # 'p_start': 0.95,\n",
    "        # 'p_end': 1,\n",
    "\n",
    "        'prices': True,\n",
    "        'transact_times': False,\n",
    "        'preds': False,\n",
    "        'smoothed_preds': False,\n",
    "        'signal': False, \n",
    "        'port_val_ts': False,\n",
    "        'y_train_rti': True,\n",
    "        'ideal_top_bottoms': False,\n",
    "    }\n",
    "    \n",
    "    # temp plot dict\n",
    "    plot_dict = {\n",
    "        # 'signal': vwap_dict['BTC-TUSD'],\n",
    "                 'preds': vwap_dict['BTC-TUSD'], \n",
    "    'y_train_rti': y, \n",
    "    }\n",
    "\n",
    "    fig = plot_framework(plot_requests=plot_requests,\n",
    "                         prices=vwap_dict['BTC-TUSD'],\n",
    "                         signal_dict=plot_dict,\n",
    "                         downsample_n=20_000,\n",
    "                         ideal_bottoms=None,\n",
    "                         ideal_tops=None, )\n",
    "\n",
    "    fig.show()\n",
    "    \n",
    "print(f\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e5dab6-d786-40fe-8166-ba0ef9c31a60",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "one_run_params['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c7f78ca-3650-4ecb-bddc-b71622de7cdc",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "[back to top](#outline_algos_framework)\n",
    "\n",
    "<div id='separate_model_run'> </div>\n",
    "\n",
    "# separate model run, each part done individually\n",
    "\n",
    "<div id='model_train_and_preds'> </div>\n",
    "\n",
    "### model training and prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56af0bb0-8e3d-4cda-9ac6-d40575f9416a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# nb_params['single_run'] == True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d6b002-6327-41f4-a0e3-224c7e5e8538",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# del run_once_model \n",
    "\n",
    "if nb_params['single_run'] == True:\n",
    "    try: \n",
    "        if run_once_rolling_decision_tree_model:\n",
    "            raise ValueError\n",
    "    except NameError:\n",
    "        pass\n",
    "    run_once_rolling_decision_tree_model = True\n",
    "\n",
    "    dataset_dict = split_train_test(x, 0.98, 0.01, 0.004)  # OVERRIDE FOR SAVING SIGNAL TO RECREATE BATCHED SIGNAL RESULTS\n",
    "    # dataset_dict = split_train_test(x, \n",
    "    #                                 one_run_params['targets']['p_train'], \n",
    "    #                                 one_run_params['targets']['p_test'], \n",
    "    #                                 one_run_params['targets']['p_validate'])\n",
    "        \n",
    "    signal_dict = rolling_decision_tree_train(prices=vwap,\n",
    "                                         x_train=dataset_dict['x']['train'],\n",
    "                                         x_test=dataset_dict['x']['test'],\n",
    "                                         target_params=one_run_params['targets'],\n",
    "                                         model_params=one_run_params['model'], ) \n",
    "    \n",
    "print(f\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84601a89-119c-4f62-8fb6-ff23df03e9a7",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if nb_params['single_run'] == True:\n",
    "    # ### PLOTTING SIGNAL\n",
    "    #\n",
    "    plot_requests = {\n",
    "        # 'start_date': (2020, 5, 10),  # ###PAUL TODO: add start / end functionality to here \n",
    "        # 'end_date': (2020, 9, 30),\n",
    "        # 'p_start': 0.001,\n",
    "        # 'p_end': 0.5,\n",
    "\n",
    "        'prices': True,\n",
    "        'transact_times': False,\n",
    "        'preds': False,\n",
    "        'smoothed_preds': True,\n",
    "        'signal':True, \n",
    "        'port_val_ts': False,\n",
    "        'y_train_rti': True,\n",
    "        'ideal_top_bottoms': False,\n",
    "    }\n",
    "\n",
    "    fig = plot_framework(plot_requests=plot_requests,\n",
    "                         prices=vwap_dict['BTC-TUSD'],\n",
    "                         signal_dict=signal_dict,\n",
    "                         downsample_n=20_000,\n",
    "                         ideal_bottoms=None,\n",
    "                         ideal_tops=None, )\n",
    "\n",
    "    # fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475f6fa1-3be5-4846-ad74-df0f88c4d742",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a728aa-150b-4836-a9f9-85f606c9ef7d",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "[back to top](#outline_algos_framework)\n",
    "\n",
    "<div id='decision_algorithm'> </div>\n",
    "\n",
    "# decision algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fbe8874-9846-4b17-a4d4-5c2f1b8b9397",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "one_run_params['decision']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80571264-75a7-4b98-9dde-71a32d4fb4c7",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "one_run_params['decision'] = {  # TODO: implement in `algos/data/live/ports/decision_params.json\n",
    "                                       'fee': 0.01,  # TODO: need to grab from port name\n",
    "                                       'max_workers': 70,\n",
    "                                       'cool_down': 15,\n",
    "                                       'threshold': -0.09999999999999998,\n",
    "                                       'pred_dist': 0.25,\n",
    "                                       'price_dist': 0.0185,\n",
    "                                       'stop_limit': 0.045,\n",
    "                                       'overrides': ['stop_limit'],\n",
    "                                       'any_two': [],\n",
    "                                       'to_neutral_threshold': 0.375,\n",
    "                                       'to_neutral_pred_dist': 0.14999999999999997,\n",
    "                                       'to_neutral_price_dist': 0.0022500000000000003,\n",
    "                                       'to_neutral_stop_limit': 0.0205,\n",
    "                                       'to_neutral_overrides': ['stop_limit'],\n",
    "                                       'to_neutral_any_two': []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f0b64d-c079-4e75-9548-a5387c3fb4ed",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%autoreload\n",
    "from algos.decision import backtest_decision_making\n",
    "nb_params= {'single_run':True}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3514ff8-b5ce-4107-aa64-8eaedfedd0c9",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# preds = y_pred\n",
    "\n",
    "if nb_params['single_run']: \n",
    "    framework_results = backtest_decision_making(price_series=vwap_dict['BTC-TUSD']['vwap'],\n",
    "                                                 signal_dict=signal_dict,\n",
    "                                                 decision_params=one_run_params['decision'])\n",
    "print(f\"done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322c60d2-5c12-40b8-8df7-cc7285004564",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "* #### summarize results of the run "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da0e6ae9-c404-4bb6-ac82-ed7b10d9da7d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if nb_params['single_run']: \n",
    "    short_count = 0 \n",
    "    long_count = 0 \n",
    "    exit_short_count = 0 \n",
    "    exit_long_count = 0 \n",
    "\n",
    "    transacts_list = framework_results['transacts_list']\n",
    "    \n",
    "    for d in transacts_list:\n",
    "        if 'short' == d['action']:\n",
    "            short_count += 1\n",
    "        if 'long' == d['action']:\n",
    "            long_count += 1\n",
    "        if 'exit_short' == d['action']:\n",
    "            exit_short_count += 1\n",
    "        if 'exit_long' == d['action']:\n",
    "            exit_long_count += 1\n",
    "\n",
    "    print(f\"short_count: {short_count} \\n\"\n",
    "          f\"long_count: {long_count} \\n\"\n",
    "          f\"exit_short_count: {exit_short_count} \\n\"\n",
    "          f\"exit_long_count: {exit_long_count} \\n\")\n",
    "    \n",
    "    # print(f\"framework_results.keys() --> {framework_results.keys()}\")\n",
    "    \n",
    "    print(f\"pnl   ----------> {framework_results['pnl']:.4f} \\n\"\n",
    "          f\"sharpe:  -------> {framework_results['sharpe']:.4f} \\n\"\n",
    "          f\"sortino:   -----> {framework_results['sortino']:.4f} \\n\"\n",
    "          f\"num_transacts: -> {framework_results['num_transacts']} \\n\"\n",
    "         )\n",
    "\n",
    "    # framework_results['port_value_ts'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3cacd7b-b0d9-4ce7-a809-f3cb084b3316",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a936adc-f63f-4c7a-b1fd-57c2d5b93d1c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if nb_params['single_run']: \n",
    "    \n",
    "    plot_requests = {\n",
    "        # 'start_date': (2023, 3, 12),  # ###PAUL TODO: add start / end functionality to here \n",
    "        # 'end_date': (2023, 3, 14),\n",
    "        # 'start_date': (2021, 6, 30),  # ###PAUL TODO: add start / end functionality to here \n",
    "        # 'end_date': (2022, 1, 1),\n",
    "        # 'p_start': 0.9,\n",
    "        # 'p_end': 0.95,\n",
    "        \n",
    "        'prices': True,\n",
    "        'transact_times': False,\n",
    "        'preds': False,\n",
    "        'smoothed_preds': False,\n",
    "        'signal': True,\n",
    "        'port_val_ts': True,\n",
    "        'y_train_rti': False,\n",
    "        'ideal_top_bottoms': False,\n",
    "    }\n",
    "\n",
    "    fig = plot_framework(plot_requests=plot_requests,\n",
    "                         prices=vwap_dict['BTC-TUSD']['vwap'],\n",
    "                         # signal_dict = signal_dict,\n",
    "                         framework_results=framework_results,\n",
    "                         downsample_n=10_000,\n",
    "                         ideal_bottoms=None,\n",
    "                         ideal_tops=None, )\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c571490-f8d4-47aa-9040-577949a837a1",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "framework_results['port_value_ts']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0303a0a-3ead-49dd-8ba0-6ca021f57aa3",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "vwap_dict['BTC-TUSD']['vwap']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dabad378-67c5-4c0c-bdc5-4a7e054e012b",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "* #### DEBUG TOOL: verifying that transactions are plotted propperly "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e389dca6-cac0-46c1-8878-76d34c970011",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# from algos.utils import convert_date_format\n",
    "\n",
    "# relevant_transacts = []\n",
    "# for transact in transacts_list:\n",
    "#     if transact['datetime'] < convert_date_format((2020, 5, 8), 'pandas'):\n",
    "#         continue\n",
    "#     elif transact['datetime'] > convert_date_format((2020, 5, 12), 'pandas'):\n",
    "#         break\n",
    "#     relevant_transacts.append(transact)\n",
    "# relevant_transacts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caeae932-adb7-4418-8cad-98e7e1357cd3",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "[back to top](#outline_algos_framework) \n",
    "\n",
    "\n",
    "<div id='grid_search'> </div>\n",
    "\n",
    "# grid search\n",
    "* ### over [feature, targets, model, decision] params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e01819-011d-4be0-a68a-1a2515047fb9",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "gs_df.sort_values(by='ranking_metric').iloc[-10:][show_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc7cad1-b1a3-4fb0-ae6e-13986d8b33a6",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# chat GPT development for hyperparemeter optimization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31291b1-2b23-4488-8da4-5661e9fab281",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pip install scikit-optimize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e68c01-0a1f-4d7f-b2dd-0557be925978",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "gs_df[show_cols].sort.s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda4a806-ebe2-4c42-9490-6ef0897eb574",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "bayes_df = deepcopy(gs_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9e4f0d-3ded-467b-87c4-0c7b47a8fd50",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7419e358-1c80-4404-b835-149bcd408287",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "params_to_opt = ['decision_cool_down',\n",
    "        'decision_threshold',\n",
    "        'decision_pred_dist',\n",
    "        'decision_price_dist',\n",
    "        'decision_stop_limit',\n",
    "        'decision_to_neutral_threshold',\n",
    "        'decision_to_neutral_pred_dist',\n",
    "        'decision_to_neutral_price_dist',\n",
    "        'decision_to_neutral_stop_limit',\n",
    "        'ranking_metric', ] \n",
    "\n",
    "\n",
    "bayes_df = bayes_df[params_to_opt]\n",
    "\n",
    "bayes_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f09ddcd-d347-45cf-9e04-b6581cb65db6",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "gs_df['ranking_metric'] = -1 * gs_df['ranking_metric']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946ba740-9ee0-4077-aa84-4161063e81d2",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from skopt import gp_minimize\n",
    "from skopt.space import Real, Integer\n",
    "\n",
    "# Define the hyperparameter space\n",
    "space  = [Integer(1, 60, name='decision_cool_down'),\n",
    "          Real(-2.5, 2.5, name='decision_threshold'),\n",
    "          Real(0.1, 1, name='decision_pred_dist'),\n",
    "          Real(0.0025, 0.035, name='decision_price_dist'),\n",
    "          Real(0.02, 0.05, name='decision_stop_limit'),\n",
    "          Real(-1, 2, name='decision_to_neutral_threshold'),\n",
    "          Real(0.05, 1, name='decision_to_neutral_pred_dist'),\n",
    "          Real(0.0025, 0.03, name='decision_to_neutral_price_dist'),\n",
    "          Real(0.00375, 0.0375, name='decision_to_neutral_stop_limit'),]\n",
    "\n",
    "\n",
    "# Define the objective function\n",
    "def objective(values):\n",
    "    import pdb\n",
    "    pdb.set_trace() \n",
    "    # Extract the names of the parameters from the space\n",
    "    param_names = [dimension.name for dimension in space]\n",
    "    params = {name: value for name, value in zip(param_names, values)}\n",
    "    \n",
    "    score = bayes_df.loc[(bayes_df[list(params.keys())] == pd.Series(params)).all(axis=1), 'ranking_metric'].mean()\n",
    "    \n",
    "    return -score  # we return the negative score because gp_minimize minimizes the function\n",
    "\n",
    "# def objective(values):\n",
    "#     params = {dim.name: value for dim, value in zip(space, values)}\n",
    "#     try:\n",
    "#         import\n",
    "#         score = run_model_with_params(params)  # this should return the score you are trying to maximize\n",
    "#         if np.isnan(score) or np.isinf(score):\n",
    "#             print(f\"Score is NaN or inf for params: {params}\")\n",
    "#             score = np.finfo(np.float64).max  # assign a large value\n",
    "#     except Exception as e:\n",
    "#         print(f\"Exception during model run for params: {params}, exception: {str(e)}\")\n",
    "#         score = np.finfo(np.float64).max  # assign a large value\n",
    "#     return -score  # we return the negative score because gp_minimize minimizes the function\n",
    "\n",
    "\n",
    "# Run the optimizer\n",
    "res = gp_minimize(objective, space, n_calls=50, random_state=0)\n",
    "\n",
    "# The optimal parameters are in res.x\n",
    "print('Best parameters:', res.x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9f514a-9ade-4a99-bf82-71c6c4b4ce96",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "bayes_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ae546d-dea5-4be9-a1c5-5a94884b26b9",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "bayes_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b877e3-0938-49af-90b6-19b513291264",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e74a57ff-8283-4016-bc08-8f84d96e5999",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# recent grid search on 5% peak signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee46e156-bd26-4875-a08c-ffe78b580b77",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if nb_params['gridsearch_run'] == True:\n",
    "    gs_n = 4\n",
    "\n",
    "    threshold = 0.2\n",
    "    pred_dist = 0.15\n",
    "    price_dist = 0.012\n",
    "    stop_limit = 0.04\n",
    "    # deltas\n",
    "    delta_threshold = 0.9\n",
    "    delta_pred_dist = 0.1\n",
    "    delta_price_dist = 0.0065\n",
    "    delta_stop_limit = 0.005\n",
    "\n",
    "    exit_threshold = 0.875\n",
    "    exit_pred_dist = 0.35\n",
    "    exit_price_dist = 0.0085\n",
    "    exit_stop_limit = 0.01625\n",
    "    # deltas\n",
    "    delta_exit_threshold = 0.5\n",
    "    delta_exit_pred_dist = 0.2\n",
    "    delta_exit_price_dist = 0.00625\n",
    "    delta_exit_stop_limit = 0.00425\n",
    "    \n",
    "\n",
    "    threshold_min = threshold - delta_threshold # max(0, threshold - delta_threshold)  \n",
    "    pred_dist_min = max(0, pred_dist - delta_pred_dist)\n",
    "    price_dist_min = max(0, price_dist - delta_price_dist)\n",
    "    stop_limit_min = max(0, stop_limit - delta_stop_limit)\n",
    "\n",
    "    exit_threshold_min = exit_threshold - delta_exit_threshold #  max(0, exit_threshold - delta_exit_threshold)  # maybe should try letting this go negative\n",
    "    # exit_threshold_min = max(0, exit_threshold - delta_exit_threshold)  # maybe should try letting this go negative\n",
    "    exit_pred_dist_min = max(0, exit_pred_dist - delta_exit_pred_dist)\n",
    "    exit_price_dist_min = max(0, exit_price_dist - delta_exit_price_dist)\n",
    "    exit_stop_limit_min = max(0, exit_stop_limit - delta_exit_stop_limit)\n",
    "\n",
    "\n",
    "    threshold_max = threshold + delta_threshold\n",
    "    pred_dist_max = pred_dist + delta_pred_dist\n",
    "    price_dist_max = price_dist + delta_price_dist\n",
    "    stop_limit_max = stop_limit + delta_stop_limit\n",
    "    exit_threshold_max = exit_threshold + delta_exit_threshold\n",
    "    exit_pred_dist_max = exit_pred_dist + delta_exit_pred_dist\n",
    "    exit_price_dist_max = exit_price_dist + delta_exit_price_dist\n",
    "    exit_stop_limit_max = exit_stop_limit + delta_exit_stop_limit\n",
    "\n",
    "    decision_dict = \\\n",
    "        {\n",
    "            'fee': [0.01],  # FEE IN PERCENT. \n",
    "            # 'preds_out_ewm': [0.45, 0.575],  # ###PAUL this goes to model\n",
    "            'max_workers': [80],\n",
    "            'cool_down': [10, 45],\n",
    "\n",
    "            'threshold': list(np.linspace(start=threshold_min, stop=threshold_max, num=gs_n)),\n",
    "            'pred_dist': list(np.linspace(start=pred_dist_min, stop=pred_dist_max, num=gs_n)),\n",
    "            'price_dist': list(np.linspace(start=price_dist_min, stop=price_dist_max, num=gs_n-1)),\n",
    "            'stop_limit': list(np.linspace(start=stop_limit_min, stop=stop_limit_max, num=gs_n-1)),\n",
    "            # 'stop_limit': [0.0088, 0.01, 0.015, ],\n",
    "            'overrides': [['stop_limit'], ],\n",
    "            'any_two': [[]], # , ['threshold', 'pred_dist', 'price_dist']],\n",
    "\n",
    "            'to_neutral_threshold': list(np.linspace(start=exit_threshold_min, stop=exit_threshold_max, num=gs_n)),\n",
    "            'to_neutral_pred_dist': list(np.linspace(start=exit_pred_dist_min, stop=exit_pred_dist_max, num=gs_n)),\n",
    "            'to_neutral_price_dist': list(np.linspace(start=exit_price_dist_min, stop=exit_price_dist_max, num=gs_n-1)),\n",
    "            'to_neutral_stop_limit': list(np.linspace(start=exit_stop_limit_min, stop=exit_stop_limit_max, num=gs_n-1)),\n",
    "            # 'to_neutral_stop_limit': [0.005, 0.009, 0.0125, ],\n",
    "            'to_neutral_overrides': [['stop_limit'], ],\n",
    "            'to_neutral_any_two': [[]], # , ['threshold', 'pred_dist', 'price_dist'], ],  or ['pred_dist', 'price_dist']\n",
    "        }\n",
    "\n",
    "print(f\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929d4ff6-01d0-4a5f-8bc2-665a88d323cd",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "if nb_params['gridsearch_run'] == True:\n",
    "    try:\n",
    "        if run_once_grid_search:\n",
    "            raise ValueError\n",
    "    except NameError:\n",
    "        run_once_grid_search = True\n",
    "        \n",
    "\n",
    "    gridsearch_params = \\\n",
    "        {\n",
    "            'features':\n",
    "                {\n",
    "                    # ###PAUL TODO: \n",
    "                }, \n",
    "            'targets':\n",
    "                {\n",
    "                    'p_train': [0.01],\n",
    "                    'p_test': [0.01], # temp to speed things up [0.3],\n",
    "                    'p_validate': [0.001],\n",
    "                    'min_peak_dist': [25],\n",
    "                    'peak_prominence': [0.04, ],  # 0.06, 0.08],\n",
    "                    'interval_len_in_steps': [14400],\n",
    "                    'smoothing_window': [None],\n",
    "                    'pd_ewm_alpha': [0.025],\n",
    "                    'training_days': [630],  # ###PAUL training days should be removed model, all rely on targets dict \n",
    "                },\n",
    "            'model':\n",
    "                {\n",
    "                    # light gbm params # ###PAUL eventuall move seprate out model params to generalizem more\n",
    "                    \"model\": [None],\n",
    "                    \"learning_rate\": [0.01],\n",
    "                    \"max_depth\": [10],\n",
    "                    \"num_leaves\": [6],\n",
    "                    \"n_estimators\": [88],\n",
    "                    \"verbose\": [-1],  # silences the model built in\n",
    "\n",
    "                    # backtest_rolling params\n",
    "                    'training_days': [630],\n",
    "                    'f_window': [0],\n",
    "                    'step_size': [5],\n",
    "                    'preds_pd_ewm_alpha': [0.025],\n",
    "                    'rolling_normalize_preds': [True],\n",
    "                    'signal_norm_window': [5760],\n",
    "                },\n",
    "\n",
    "            'decision': decision_dict\n",
    "        }\n",
    "\n",
    "    print(f\"STARTING GRIDSEARCH\")\n",
    "    # results_df = grid_search_over_parameters(x, prices['vwap'], gridsearch_params, )\n",
    "\n",
    "print(f\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91afa425-0c59-4219-9bf8-55c2634d6585",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "* ### grid search over `signal_dict`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3ada6a-de5d-4890-ac79-4f8182d41d73",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from algos.gridsearch import make_list_of_combo_dicts, decision_multiprocess\n",
    "\n",
    "target_params_nn_temp = {'not_ready_for_nn': 'skip'}\n",
    "model_params_nn_temp = {'not_ready_for_nn': 'skip'}\n",
    "decision_combos = make_list_of_combo_dicts(gridsearch_params['decision'])\n",
    "                     \n",
    "gs_df = decision_multiprocess(decision_combos=decision_combos,\n",
    "                                signal_dict=signal_dict,\n",
    "                                prices=vwap,\n",
    "                                target_params=target_params_nn_temp,\n",
    "                                model_params=model_params_nn_temp,\n",
    "                                max_workers=80)\n",
    "\n",
    "gs_df['ranking_metric'] = gs_df['sharpe'] * gs_df['pnl'] / gs_df['num_transacts' ] \n",
    "\n",
    "print(f\"done\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b03144-b3dc-43bd-92be-6488c984f81d",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div id='one_model_run'> </div>\n",
    "\n",
    "# one model run\n",
    "* #### check `one_run_params`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3362e1a0-de67-482b-9de9-bd6eb0f2eda3",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if nb_params['one_full_run'] == True:\n",
    "    try:\n",
    "        if run_once_single_model:\n",
    "            raise ValueError\n",
    "    except NameError:\n",
    "        run_once_single_model = True\n",
    "\n",
    "    framework_results = run_full_process(x, prices['vwap'], run_on_validation_period=False, **one_run_params)\n",
    "    drawdowns, recoveries = analyze_framework_run(framework_results=framework_results, ts_freq='min')\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e5ef35-ed14-4001-a4ab-c249213f508f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if nb_params['one_full_run'] == True:\n",
    "    plot_requests = {\n",
    "        # 'start_date': (2020, 4, 20),  # ###PAUL TODO: add start / end functionality to here \n",
    "        # 'end_date': (2020, 4, 23),\n",
    "        # 'p_start': 0.0,\n",
    "        # 'p_end': 3/7,\n",
    "\n",
    "        'prices': True,\n",
    "        'transact_times': True,\n",
    "        'preds': False,\n",
    "        'smoothed_preds': True,\n",
    "        'port_val_ts': True,\n",
    "        'y_train_rti': False,\n",
    "        'ideal_top_bottoms': False,\n",
    "    }\n",
    "\n",
    "    fig = plot_framework(plot_requests=plot_requests,\n",
    "                         prices=prices['vwap'],\n",
    "                         framework_results=framework_results,\n",
    "                         downsample_n=15_000,\n",
    "                         ideal_bottoms=None,\n",
    "                         ideal_tops=None, )\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76733423-48a4-4359-87e8-4a388173a43e",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "[back_to_top](#outline_algos_framework) \n",
    "\n",
    "\n",
    "<div id='neural_net_dev'> </div> \n",
    "\n",
    "* # neural_net_dev\n",
    "    * [mlp - batched](#mlp_batched)\n",
    "    * [mlp - rolling](#mlp_rolling)\n",
    "\n",
    "<div id='mlp_batched'> </div> \n",
    "\n",
    "## batched mlp \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81b71b0-d271-4876-996d-44665df5f819",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# del run_once_norm_whole_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24f48f5-ee0e-408d-a57d-0ae6a31d55a2",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "if nb_params['MLP_batch']: \n",
    "    try:\n",
    "        if run_once_norm_whole_dataset:\n",
    "            raise ValueError\n",
    "    except NameError:\n",
    "        run_once_norm_whole_dataset = True\n",
    "\n",
    "    # normalize the feature set in a rolling manner.. \n",
    "    rolling_normalization_n_obvs = 30*24*60\n",
    "    x_normed_rti = ((x - x.rolling(rolling_normalization_n_obvs).mean()).fillna(0) /\n",
    "                    x.rolling(rolling_normalization_n_obvs).std().fillna(method='ffill')).iloc[rolling_normalization_n_obvs-1:]\n",
    "    y_normed_rti = ((y - y.rolling(rolling_normalization_n_obvs).mean()) /\n",
    "                    y.rolling(rolling_normalization_n_obvs).std().fillna(method='ffill')).iloc[rolling_normalization_n_obvs-1:]\n",
    "\n",
    "    # get rid of columns without enough data \n",
    "    nan_count_by_col = x_normed_rti.isna().sum()\n",
    "    over_thresh = 0.05 < nan_count_by_col/x_normed_rti.shape[0] # identify all columns with more than 5% missing \n",
    "    x_normed_rti = x_normed_rti[over_thresh.index[~over_thresh]]\n",
    "\n",
    "    x_normed_rti = x_normed_rti.fillna(method='ffill').fillna(method='bfill').fillna(value=0)\n",
    "    y_normed_rti = y_normed_rti.fillna(method='ffill').fillna(method='bfill').fillna(value=0)\n",
    "\n",
    "    x_normed_rti = np.clip(a=x_normed_rti, a_min=-10, a_max=10)\n",
    "    y_normed_rti = np.clip(a=y_normed_rti, a_min=-10, a_max=10)\n",
    "\n",
    "    x_normed_rti, y_normed_rti = cut_two_pd_dti_objects_to_matching_dtis(x_normed_rti, y_normed_rti)\n",
    "\n",
    "    print(f\"CHECKING ARRAYS (sometimes weird infs / -infs  pop up)\")\n",
    "    print(f\"min(x_normed_rti) ---- {np.min(x_normed_rti.values)}\")\n",
    "    print(f\"max(x_normed_rti) ---- {np.max(x_normed_rti.values)}\")\n",
    "    print(f\"mean(x_normed_rti) ---- {(x_normed_rti.values).mean()}\")\n",
    "    print(f\"std(x_normed_rti) ---- {(x_normed_rti.values).std()}\")\n",
    "    print(f\"min(y_normed_rti) ---- {min(y_normed_rti)}\")\n",
    "    print(f\"max(y_normed_rti) ---- {max(y_normed_rti)}\")\n",
    "    print(f\"mean(y_normed_rti) ---- {(y_normed_rti.values).mean()}\")\n",
    "    print(f\"std(y_normed_rti) ---- {(y_normed_rti.values).std()}\")\n",
    "\n",
    "print(f\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88882639-3dc2-4b33-bd9f-6b6d6df4a0cf",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "* #### create batched model for training \n",
    "    * split in this cell because cleaning above will affect model via `feature_in_dim`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63386f19-22d6-4af5-b2f3-dc6303a1f197",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# override_model = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6435ea-0066-4e84-974c-3cf3b9885c44",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "if nb_params['MLP_batch']:\n",
    "    # dataset_dict = split_train_test(x_normed_rti, 0.50, 0.15, 0.01)    # <<---- TEST PERIOD\n",
    "    # dataset_dict = split_train_test(x_normed_rti, 0.65, 0.15, 0.005)    # <<---- TEST PERIOD\n",
    "    dataset_dict = split_train_test(x_normed_rti, 0.55, 0.35, 0.005)    # <<---- TEST PERIOD ---- THIS IS A TEMP NUMBER TO GET A SIGNAL FOR 2021 FOR YOHAN     \n",
    "    # dataset_dict = split_train_test(x_normed_rti, 0.8, 0.195, 0.005)    # <<---- TEST PERIOD ---- THIS IS A TEMP NUMBER TO GET A SIGNAL FOR 2022 FOR YOHAN \n",
    "    # dataset_dict = split_train_test(x_normed_rti, 0.8, 0.15, 0.01)   # <<---- VALIDATE PERIOD\n",
    "    feature_in_dim = dataset_dict['x']['train'].shape[1]\n",
    "    \n",
    "    # make dataloaders for batched training \n",
    "    train_dataloader = make_mlp_dataloader(x_in=dataset_dict['x']['train'], batch_size=24*60, y_in=y_normed_rti, shuffle=False) \n",
    "    test_dataloader = make_mlp_dataloader(dataset_dict['x']['test'], batch_size=24*60, y_in=y_normed_rti, shuffle=False) \n",
    "    test_dataloader_no_targets = make_mlp_dataloader(dataset_dict['x']['test'], batch_size=24*60, y_in=None, shuffle=False)\n",
    "    \n",
    "    # dataset_dict = split_train_test(x_normed_rti, 0.65, 0.15, 0.15)   \n",
    "    # dataset_dict = split_train_test(x_normed_rti, 0.8, 0.075, 0.075)   \n",
    "    # train_dataloader = make_mlp_dataloader(x_in=dataset_dict['x']['test'], batch_size=24*60, y_in=y_normed_rti, shuffle=False) \n",
    "    # test_dataloader = make_mlp_dataloader(dataset_dict['x']['validate'], batch_size=24*60, y_in=y_normed_rti, shuffle=False) \n",
    "    # test_dataloader_no_targets = make_mlp_dataloader(dataset_dict['x']['validate'], batch_size=24*60, y_in=None, shuffle=False)\n",
    "\n",
    "    # Get cpu or gpu device for training.\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print('using ---> ' + device)\n",
    "\n",
    "    # ### adjustable parameters\n",
    "    #\n",
    "    #\n",
    "    # loss functions \n",
    "    loss_fn = nn.MSELoss()\n",
    "    # loss_fn = nn.L1Loss()\n",
    "    \n",
    "    try:\n",
    "        if override_model == True:\n",
    "            print(f\"overrided model weights\")\n",
    "            model = NeuralNetwork(feature_in_dim=feature_in_dim).to(device)\n",
    "        else:\n",
    "            print(f\"model weights left unchanged\") \n",
    "    except NameError:\n",
    "        try:\n",
    "            model\n",
    "            print(f\"model weights left unchanged\") \n",
    "        except NameError:\n",
    "            print(f\"model did not exist, so made one\")\n",
    "            model = NeuralNetwork(feature_in_dim=feature_in_dim).to(device)\n",
    "    override_model = False\n",
    "        \n",
    "    # optimizer = torch.optim.SGD(model.parameters(), lr=1e-4)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "    torch.set_num_threads(80)\n",
    "\n",
    "    print(model)\n",
    "    \n",
    "print(f\"done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc0b055-4f09-4993-bd18-a2e89543c267",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "* #### run batched training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e2d813-a9e2-41fa-96a1-b470fd3a8606",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "run_once_mlp_batch_train = True \n",
    "del run_once_mlp_batch_train\n",
    "    \n",
    "if nb_params['MLP_batch']:\n",
    "    try: \n",
    "        if run_once_mlp_batch_train == True:\n",
    "            raise ValueError\n",
    "    except NameError:\n",
    "        pass\n",
    "    run_once_mlp_batch_train = True\n",
    "\n",
    "    # ## set number of epochs\n",
    "    #\n",
    "    epochs = 10\n",
    "    \n",
    "    for t in range(epochs):\n",
    "        print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "        torch.set_num_threads(80)\n",
    "        train(train_dataloader, model, loss_fn, optimizer, printout_level='Low')\n",
    "        torch.set_num_threads(80)\n",
    "        test_loss = test(test_dataloader, model, loss_fn)\n",
    "        print(f\"-- test_loss: {test_loss}\") \n",
    "        \n",
    "print(\"done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdfb279b-c8ff-436d-81a3-b07684f5f963",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "* #### create predictions for batch of observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6d90cc-322e-42c7-b808-ac6c295acd12",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if nb_params['MLP_batch']:\n",
    "    \n",
    "    # ###PAUL TODO: handle this better ---- used between batch and rolling\n",
    "    model_params = \\\n",
    "        {\n",
    "            # ROLLING train / test functionality only \n",
    "            'train_days': 77,\n",
    "            'step_size': 14,\n",
    "            'f_window': 0,\n",
    "            'num_epochs_for_train': 20,\n",
    "            'num_epochs_when_rolling': 4,\n",
    "            'batch_size': 24 * 60, \n",
    "\n",
    "            # ###PAUL TODO: build the target post processing into `make_targets()` adding  `target_params` as input\n",
    "            'volatility_normalization_for_targets': False,\n",
    "            'return_window_n': 2*60,         # REQUIRED IF `volatility_normalization_for_targets` == True\n",
    "            'variance_window_n': 2*24 * 60,  # REQUIRED IF `volatility_normalization_for_targets` == True\n",
    "        \n",
    "            # make target series used for training have standard normal distribution \n",
    "            'normalize_targets': True, \n",
    "\n",
    "            # 'optimizer': torch.optim.SGD(model.parameters(), lr=1e-4),\n",
    "            'optimizer': torch.optim.Adam(model.parameters(), lr=1e-3),\n",
    "            # 'loss_fn': nn.L1Loss(),\n",
    "            'loss_fn': nn.MSELoss(),\n",
    "        \n",
    "            # dataloader shuffle stuff (planned for batch training, but may want to add to rolling) \n",
    "            'shuffle_train': False,\n",
    "            'shuffle_test': False,  # this should always be False, maybe dont give it an argument\n",
    "\n",
    "            # ### preds post processing parameters to make preds into a signal \n",
    "            'rolling_normalize_preds': True,\n",
    "            'preds_pd_ewm_alpha': 0.01,\n",
    "            'signal_norm_window': 5*24*60,   # REQUIRED IF `rolling_normalize_preds` == True\n",
    "        }\n",
    "    \n",
    "    # preds = make_predictions_for_dataloader_features_only(test_dataloader_no_targets, model)\n",
    "    # preds = reshape_preds(preds)\n",
    "    # preds = pd.Series(preds, index=dataset_dict['x']['test'].index)\n",
    "    # # preds = pd.Series(preds, index=dataset_dict['x']['validate'].index)\n",
    "        \n",
    "    signal_dict = make_signal_dict_from_preds(\n",
    "                                              # preds, \n",
    "                                              signal_dict['preds'],\n",
    "                                              y_train_rti=y_normed_rti, \n",
    "                                              target_params=one_run_params['targets'],\n",
    "                                              model_params=model_params,\n",
    "                                              model=model, \n",
    "                                              # model=signal_dict['model'], \n",
    "                                             )\n",
    "\n",
    "print(f\"done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d77740-f488-41c8-9e42-4be79ad94617",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "[back to top](#outline_algos_framework) \n",
    "\n",
    "<div id='mlp_rolling'> </div> \n",
    "\n",
    "* # rolling MLP train / test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4f3260-97e9-4bd2-b9da-45f5b21e55b2",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "del run_once_norm_whole_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15942fd6-7a5d-44d2-a7ab-47208126c689",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047177a5-f5f6-46df-8bd0-6e0e2194a070",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "if nb_params['MLP_rolling'] == True:\n",
    "    try:\n",
    "        if run_once_norm_whole_dataset:\n",
    "            raise ValueError\n",
    "    except NameError:\n",
    "        run_once_norm_whole_dataset = True\n",
    "\n",
    "    # normalize the feature set in a rolling manner.. \n",
    "    preprocess_rolling_norm_n_obvs = one_run_params['feature']['preprocess_rolling_norm_n_obvs']\n",
    "    x_normed_rti = ((x - x.rolling(preprocess_rolling_norm_n_obvs).mean()).fillna(0) /\n",
    "                    x.rolling(preprocess_rolling_norm_n_obvs).std().fillna(method='ffill')).iloc[\n",
    "                   preprocess_rolling_norm_n_obvs - 1:]\n",
    "\n",
    "    # get rid of columns without enough data \n",
    "    nan_count_by_col = x_normed_rti.isna().sum()\n",
    "    over_thresh = 0.05 < nan_count_by_col / x_normed_rti.shape[0]  # identify all columns with more than 5% missing \n",
    "    x_normed_rti = x_normed_rti[over_thresh.index[~over_thresh]]\n",
    "\n",
    "    x_normed_rti = x_normed_rti.fillna(method='ffill').fillna(method='bfill').fillna(value=0)\n",
    "    x_normed_rti = np.clip(a=x_normed_rti, a_min=-10, a_max=10)\n",
    "\n",
    "    feature_in_dim = x_normed_rti.shape[1]\n",
    "\n",
    "    # og_x_normed_rti = deepcopy(x_normed_rti)  # ###PAUL_del_later make a copy of x_normed for now \n",
    "\n",
    "    print(f\"done\")\n",
    "\n",
    "# # ### saving preprocessed features in cache (incase something happens) \n",
    "# x_normed_rti = deepcopy(og_x_normed_rti)  ### bring back the original x normed easily "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014040ec-db9c-4676-860a-a44faa5eefed",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "check_prices_for_gaps(x_normed_rti)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a12a6a4-31be-4fe4-9130-5392241aec43",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "x_normed_rti.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab19f57-57a7-46c4-b55f-733d21a569ed",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "x_normed_rti.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822e1656-ce2d-4560-939f-d72728d5efa7",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# rolling_normed_feature_set_fp =  f\"{params['dirs']['data_dir']}features/rolling_normed_30_d.pickle\"\n",
    "# # x_normed_rti.to_pickle(rolling_normed_feature_set_fp)\n",
    "# x_normed_rti = pd.read_pickle(rolling_normed_feature_set_fp)\n",
    "# vwap_prices_fp = f\"{params['dirs']['data_dir']}prices/just_vwap.pickle\"\n",
    "# # prices['vwap'].to_pickle(vwap_prices_fp)\n",
    "# vwap_prices = pd.read_pickle(vwap_prices_fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfbb68df-bc22-40a2-827d-81be33d0a6a7",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "* ### run rolling training process "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9dc3d9-f04c-4c6d-8521-4725f63b1e02",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "nb_params['MLP_rolling'] = True\n",
    "del run_once_mlp_rolling_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf1507c3-9409-4090-aaf4-43d96cc1905c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "x_normed_rti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9f9c0b-478b-4096-9371-f2d6070198e9",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "\n",
    "run_once_mlp_rolling_train = False\n",
    "if nb_params['MLP_rolling'] == True:\n",
    "    \n",
    "    try:\n",
    "        if run_once_mlp_rolling_train:\n",
    "            print(f\"`run_once_mlp_rolling_train` must be deleted or set False\")\n",
    "            raise ValueError\n",
    "        run_once_mlp_rolling_train = True\n",
    "    except NameError: \n",
    "        run_once_mlp_rolling_train = True\n",
    "        pass\n",
    "    \n",
    "    # ### data splitting\n",
    "    #\n",
    "    sd = convert_date_format((2018, 1, 1), 'pandas')  \n",
    "    ed = convert_date_format((2023, 7, 18), 'pandas')   # FOR TEST \n",
    "    # ed = convert_date_format((2023, 5, 19), 'pandas')   # FOR VALIDATION\n",
    "    x_for_roll = x_normed_rti[np.logical_and(sd < x_normed_rti.index, x_normed_rti.index < ed) ]  \n",
    "    \n",
    "    \n",
    "    feature_in_dim = x_for_roll.shape[1]                  \n",
    "      \n",
    "    # Get cpu or gpu device for training.\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print('using ---> ' + device)\n",
    "\n",
    "    # ### adjustable parameters\n",
    "    #\n",
    "    # loss functions \n",
    "    loss_fn = nn.MSELoss()\n",
    "    # loss_fn = nn.L1Loss()\n",
    "\n",
    "    model = NeuralNetwork(feature_in_dim=feature_in_dim).to(device)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=1e-4)\n",
    "    # optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "    \n",
    "    print(model)\n",
    "\n",
    "    target_params= \\\n",
    "        {\n",
    "         'min_peak_dist': 480,\n",
    "         'peak_prominence': 0.055,\n",
    "         'interval_len_in_steps': 35*24*60,\n",
    "         'pd_ewm_alpha': 0.01\n",
    "    }\n",
    "    \n",
    "\n",
    "    model_params = \\\n",
    "        {\n",
    "            'train_days': 365,\n",
    "            'step_size': 3,\n",
    "            'f_window': 0,\n",
    "\n",
    "            'num_epochs_for_train': 25,\n",
    "            'num_epochs_when_rolling': 5,\n",
    "            # 'num_epochs_for_train': 2,\n",
    "            # 'num_epochs_when_rolling': 1,\n",
    "                \n",
    "            'batch_size': 24 * 30,\n",
    "            \n",
    "            # dataloader shuffle stuff (planned for batch training, but may want to add to rolling) \n",
    "            'shuffle_train': False,\n",
    "            'shuffle_test': False,  # this should always be False, maybe dont give it an argument\n",
    "        \n",
    "            'volatility_normalization_for_targets': False,  # ###PAUL TODO: check this calculation, OFF for now\n",
    "            'return_window_n': 3*60,         # REQUIRED IF `volatility_normalization_for_targets` == True\n",
    "            'variance_window_n': 3*24*60,    # REQUIRED IF `volatility_normalization_for_targets` == True\n",
    "\n",
    "            'normalize_targets': True,  # normalizes targets per each iteration BEFORE cutting them for training\n",
    "\n",
    "            'optimizer': optimizer,\n",
    "            # 'loss_fn': nn.L1Loss(),\n",
    "            'loss_fn': nn.MSELoss(),\n",
    "       \n",
    "            # ### preds post processing parameters to make preds into a signal \n",
    "            'preds_pd_ewm_alpha': 0.01,\n",
    "            'rolling_normalize_preds': True,\n",
    "            'signal_norm_window': 5*24*60,   # REQUIRED IF `rolling_normalize_preds` == True\n",
    "           \n",
    "            # 'lstm_len': 120,  # ###PAUL this wil need to be worked into the index`\n",
    "            # # # needs to be subtracted from the start of the of the training data but more importantly the start of\n",
    "            # # the test data... implications if f_window==0...? technically the first `lstm_len` of obvservations\n",
    "            # # of the test period would contain info from the training period, though this would match real live\n",
    "            # # at midnight right after training, of course if f_window >= 1 then we have no problem\n",
    "            # # ... but this initially came into my head on how to create a continuous output series for plotting...\n",
    "            # # soo, to handle that - subtract `lstm_len` from start_test_ioc, should handle it.\n",
    "            # # also has affects on the train end because I want to continue to support outputting the training targets\n",
    "        }\n",
    "\n",
    "    print(f\"got here\")\n",
    "    signal_dict = rolling_train_and_backtest_mlp(  x=x_for_roll,\n",
    "                                                   prices=vwap_dict['BTC-TUSD']['vwap'],\n",
    "                                                   model=model,\n",
    "                                                   feature_params=feature_params, \n",
    "                                                   target_params=target_params,\n",
    "                                                   model_params=model_params,\n",
    "                                                   printout_level='Low')\n",
    "\n",
    "print(f\"done\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11053dd9-feec-40e9-8822-3aa9ea4d7950",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "[back to top](#outline_algos_framework)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9a8ebb-a7f5-42aa-a92f-84ac84469202",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "* ### signal analysis and threshold / time hold based decision framework_results\n",
    "* #### make `signal_dict` for batched method\n",
    "    * using the model to predict a signal, then some postprocessing on predictions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69a5214-eb9c-43f4-8285-034d75529776",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# single_pair_signal_analysis = analyze_signal(vwap_dict['BTC-TUSD'], single_pair_signal_dict, signal_series_name='signal', target_series_name='y_train_rti')\n",
    "# print_signal_analysis(single_pair_signal_analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6150c2-9ee0-4203-9959-5a21312f255a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "signal_analysis = analyze_signal(vwap_dict['BTC-TUSD'], signal_dict, signal_series_name='signal', target_series_name='y_train_rti')\n",
    "print_signal_analysis(signal_analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3dbd70b-ebbd-48b7-8607-1bc96580c573",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "signal_analysis = analyze_signal(vwap_dict['BTC-TUSD'], signal_dict, signal_series_name='signal', target_series_name='y_train_rti')\n",
    "print_signal_analysis(signal_analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c948b1c2-0c63-46c9-9fb4-ab4c94eb7ef9",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "neg_signal_dict = deepcopy(signal_dict) \n",
    "neg_signal_dict['signal'] = -1 * neg_signal_dict['signal']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6195243a-ae9b-425e-910e-b57fedbe9e23",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%autoreload\n",
    "from algos.decision import run_simple_signal_threshold_and_hold_time_based_framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "746144b6-5f42-49bb-b825-4269418c4a37",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "framework_results = \\\n",
    "    run_simple_signal_threshold_and_hold_time_based_framework(\n",
    "                                                              prices=vwap_dict['BTC-TUSD'],\n",
    "                                                              signal_dict=signal_dict,\n",
    "                                                              thresh=2,\n",
    "                                                              hours=5*24,\n",
    "                                                              trigger_on='back_under',\n",
    "                                                              # trigger_on='break_above',\n",
    "                                                              fee_in_percent=0,    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0e8aa8-143c-4949-ab5e-beaf8e7f4f87",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "framework_results = \\\n",
    "    run_simple_signal_threshold_and_hold_time_based_framework(\n",
    "                                                              prices=vwap_dict['BTC-TUSD'],\n",
    "                                                              signal_dict=neg_signal_dict,\n",
    "                                                              thresh=2,\n",
    "                                                              hours=20,\n",
    "                                                              # trigger_on='back_under',\n",
    "                                                              trigger_on='break_above',\n",
    "                                                              fee_in_percent=0,      \n",
    "                                                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa5ff54-cff6-418e-866c-d884f27abc67",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "neg_signal_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef34fdab-83f6-4329-99fa-f06dc0fb788d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "neg_signal_dict['signal']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ef77b9-e744-4464-bb35-08370351a8cb",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "neg_signal_dict['signal'][neg_signal_dict['signal'].index.year == 2022]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28358dd-ddf0-4bfd-8b3f-6df96ec2a179",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c26966-0f54-4af8-82a0-ffa048e048c8",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c3e0f0-a176-4da3-ab3f-9c4d6e4ec617",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d86adf8-3931-4217-8ffc-7d9242dfb6e1",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# # ### NAME OF `signal_dict` run\n",
    "# #\n",
    "# #\n",
    "# signal_dict_name = f\"signal_dict____2023_08_01____mlp_rolling____first_multi_asset_feature_set\"  \n",
    "# signal_dict_name = f\"signal_dict____2023_08_03___mlp_rolling____validate\"  \n",
    "# signal_dict_name = f\"signal_dict____2023_08_23___mlp_rolling____to_2023_07_18\"\n",
    "signal_dict_name = f\"signal_dict____2023_09_01___mlp_rolling_smaller_model\"\n",
    "# signal_dict_name = f\"signal_dict____2023_09_01___mlp_rolling____large_model\"\n",
    "# signal_dict_name = f\"signal_dict____2023_09_01___mlp_rolling____large_model_bigger_learning_rate\"\n",
    "# signal_dict_name = f\"signal_dict____2023_09_01___mlp_rolling____large_model_SGD_1e4\"\n",
    "\n",
    "signal_dict_fp = f\"{params['dirs']['data_dir']}pickled_signal_dicts/{signal_dict_name}.pickle\"\n",
    "\n",
    "# # ### SAVING\n",
    "# #\n",
    "# if os.path.isfile(signal_dict_fp): \n",
    "#     print(f\"signal already exists at that filepath\") \n",
    "# else:\n",
    "#     pickle.dump(signal_dict,  #   <<<------- COPY PASTE DESIRED VARIABLE HERE\n",
    "#                 open(signal_dict_fp, \"wb\"))\n",
    "\n",
    "\n",
    "# # ### OPENING\n",
    "# #\n",
    "with open(signal_dict_fp, 'rb') as f:\n",
    "     signal_dict = pickle.load(f) #   <<<------- COPY PASTE DESIRED VARIABLE HERE\n",
    "\n",
    "### LEAVE ON ALWAYS\n",
    "#\n",
    "del signal_dict_name; del signal_dict_fp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d319f9d3-fdff-4f57-b64d-c9632810bc43",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "neg_signal_dict = deepcopy(signal_dict) \n",
    "neg_signal_dict['signal'] = -1 * neg_signal_dict['signal']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6939ccd-dea3-4e27-9f7a-6709ab5be8f8",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "signal_dict['model']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a09bcf-1607-4751-840a-4e9a0871aaec",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "signal_dict['model']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2cf293-fde4-4315-a108-2fc6d6bbe63d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8e8ba40e-c752-4db3-aed3-9c6ce12cf265",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# results of smaller model below "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de86fab0-f6f1-4215-b571-07117bfd50b2",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# if nb_params['neural_net_dev'] == True:\n",
    "    \n",
    "#     # ### PLOTTING SIGNAL\n",
    "#     #\n",
    "#     plot_requests = {\n",
    "#         # 'start_date': (2023, 2, 1),  # ###PAUL TODO: add start / end functionality to here \n",
    "#         # 'end_date': (2023, 7, 1),\n",
    "#         # 'p_start': 0.5,\n",
    "#         # 'p_end': 0.69,\n",
    "\n",
    "#         'prices': True,\n",
    "#         'transact_times': False,\n",
    "#         'preds': False,\n",
    "#         'smoothed_preds': True,\n",
    "#         'signal': True,\n",
    "#         'port_val_ts': True,\n",
    "#         'y_train_rti': False,\n",
    "#         'ideal_top_bottoms': False,\n",
    "#     }\n",
    "\n",
    "#     fig = plot_framework(plot_requests=plot_requests,\n",
    "#                          prices=vwap_dict['BTC-TUSD']['vwap'],\n",
    "#                          framework_results=framework_results, \n",
    "#                          downsample_n=20_000, \n",
    "#                          ideal_bottoms=None, \n",
    "#                          ideal_tops=None, )\n",
    "\n",
    "#     fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee70a7aa-2f69-4ea1-8034-8fc6472d4bf0",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "years = [2019, 2020, 2021, 2022, 2023]\n",
    "port_vals_to_concat = [] \n",
    "\n",
    "temp_signal_dict = deepcopy(neg_signal_dict) \n",
    "\n",
    "\n",
    "framework_results = \\\n",
    "    run_simple_signal_threshold_and_hold_time_based_framework(\n",
    "                                                              prices=vwap_dict['BTC-TUSD'],\n",
    "                                                              signal_dict=neg_signal_dict,\n",
    "                                                              thresh=2,\n",
    "                                                              hours=20,\n",
    "                                                              # trigger_on='back_under',\n",
    "                                                              trigger_on='break_above',\n",
    "                                                              fee_in_percent=0,      \n",
    "                                                              )\n",
    "for year in years:\n",
    "    years_signal = neg_signal_dict['signal'][neg_signal_dict['signal'].index.year == year]\n",
    "    temp_signal_dict['signal'] = years_signal \n",
    "    \n",
    "    print(f\"-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\\n\"\n",
    "          f\"RUNNING BACKTEST FOR {year} \\n\")\n",
    "          \n",
    "    framework_results_t = \\\n",
    "        run_simple_signal_threshold_and_hold_time_based_framework(\n",
    "                                                                  prices=vwap_dict['BTC-TUSD'],\n",
    "                                                                  signal_dict=temp_signal_dict,\n",
    "                                                                  thresh=2,\n",
    "                                                                  hours=20,\n",
    "                                                                  # trigger_on='back_under',\n",
    "                                                                  trigger_on='break_above',\n",
    "                                                                  fee_in_percent=0,      \n",
    "                                                                  )\n",
    "    \n",
    "    port_vals_to_concat.append(framework_results_t['port_value_ts']) \n",
    "    \n",
    "    \n",
    "framework_results['port_value_ts'] = pd.concat(port_vals_to_concat) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1004169f-a381-4b34-bc21-f5509c37e0fb",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if nb_params['neural_net_dev'] == True:\n",
    "    \n",
    "    # ### PLOTTING SIGNAL\n",
    "    #\n",
    "    plot_requests = {\n",
    "        # 'start_date': (2023, 2, 1),  # ###PAUL TODO: add start / end functionality to here \n",
    "        # 'end_date': (2023, 7, 1),\n",
    "        # 'p_start': 0.5,\n",
    "        # 'p_end': 0.69,\n",
    "\n",
    "        'prices': True,\n",
    "        'transact_times': False,\n",
    "        'preds': False,\n",
    "        'smoothed_preds': True,\n",
    "        'signal': True,\n",
    "        'port_val_ts': True,\n",
    "        'y_train_rti': True,\n",
    "        'ideal_top_bottoms': False,\n",
    "    }\n",
    "\n",
    "    fig = plot_framework(plot_requests=plot_requests,\n",
    "                         prices=vwap_dict['BTC-TUSD']['vwap'],\n",
    "                         framework_results=framework_results, \n",
    "                         downsample_n=20_000, \n",
    "                         ideal_bottoms=None, \n",
    "                         ideal_tops=None, )\n",
    "\n",
    "    # fig.show()\n",
    "    \n",
    "years = [2019, 2020, 2021, 2022, 2023]\n",
    "for year in years:\n",
    "    fig.add_shape(\n",
    "        go.layout.Shape(\n",
    "            type=\"line\",\n",
    "            x0=f\"{year}-01-01\",\n",
    "            x1=f\"{year}-01-01\",\n",
    "            y0=0,\n",
    "            y1=60000,\n",
    "            # xref=\"x\",\n",
    "            yref=\"paper\",\n",
    "            line=dict(color=\"red\", width=1.5), \n",
    "            # row=1, \n",
    "            # col=1\n",
    "        ),\n",
    "        row=1, col=1\n",
    "    )\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac73042-646e-479d-9d2e-7211c8cf2abe",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "2+2**9*(11.07**(1/2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513d2014-1e90-4be2-9b02-6d7f16cacd57",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Function to process and write trades to ClickHouse\n",
    "def process_trade_data(trade):\n",
    "    # Connect to ClickHouse\n",
    "    client = Client('localhost')\n",
    "\n",
    "    # Example schema for your trade data\n",
    "    trade_data = {\n",
    "        'timestamp': trade['timestamp'],\n",
    "        'symbol': trade['symbol'],\n",
    "        'price': trade['price'],\n",
    "        'amount': trade['amount']\n",
    "    }\n",
    "\n",
    "    # Insert into ClickHouse (modify table name and columns as per your schema)\n",
    "    client.execute('INSERT INTO trades (timestamp, symbol, price, amount) VALUES', [trade_data])\n",
    "\n",
    "# Initialize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ac5a11-7ac2-47d0-acfc-085514316bcb",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Create a sample time series dataframe\n",
    "date_rng = pd.date_range(start='2019-01-01', end='2023-12-31', freq='D')\n",
    "plot_port_vals = pd.Series(np.random.randn(len(date_rng)), index=date_rng)\n",
    "plot_port_vals2 = pd.Series(np.random.randn(len(date_rng)), index=date_rng)\n",
    "\n",
    "# Create subplots\n",
    "fig = make_subplots(rows=2, cols=1)\n",
    "\n",
    "# Add scatter plots\n",
    "fig.add_trace(go.Scattergl(x=plot_port_vals.index, y=plot_port_vals, name='port_val'), row=1, col=1)\n",
    "fig.add_trace(go.Scattergl(x=plot_port_vals2.index, y=plot_port_vals2, name='port_val_2'), row=2, col=1)\n",
    "\n",
    "# Add vertical lines at the start of each year\n",
    "years = [2019, 2020, 2021, 2022, 2023]\n",
    "for year in years:\n",
    "    fig.add_shape(\n",
    "        go.layout.Shape(\n",
    "            type=\"line\",\n",
    "            x0=f\"{year}-01-01\",\n",
    "            x1=f\"{year}-01-01\",\n",
    "            y0=0,\n",
    "            y1=1,\n",
    "            xref=\"x\",\n",
    "            yref=\"paper\",\n",
    "            line=dict(color=\"red\", width=1.5),\n",
    "        ),\n",
    "        row=1,\n",
    "        col=1\n",
    "    )\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f37532-5406-40a7-b287-4ddb185a5bde",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "* #### find strategy analysis function and put here \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d35c913-6151-4267-92b8-ae6426b4707d",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "[back to top](#outline_algos_framework) \n",
    "\n",
    "<div id='neural_net_lstm'> </div> \n",
    "\n",
    "## lstm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d21cbb-9c82-4eac-9791-8a7611d806a3",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "##### reshaping \n",
    "* #### $X: \\mathbb{R}^{n, p} \\rightarrow  \\mathbb{R}^{n, p} $ \n",
    "\n",
    "* #### $1: \\mathbb{R}^{n, 1} \\rightarrow  \\mathbb{R}^{n-s, 1}$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f7d818c-e24f-45f2-8274-103c9326e136",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "resources for transition to LSTM: \n",
    "* [pytorch beginner lstm](https://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html)\n",
    "* examples\n",
    "    * [How to use PyTorch LSTMs for time series regression](https://www.crosstab.io/articles/time-series-pytorch-lstm/#evaluation)\n",
    "    * [LSTM for Time Series Prediction in PyTorch](https://machinelearningmastery.com/lstm-for-time-series-prediction-in-pytorch/)\n",
    "    * [Time Series Forecasting with Deep Learning in PyTorch (LSTM-RNN)](https://towardsdatascience.com/time-series-forecasting-with-deep-learning-in-pytorch-lstm-rnn-1ba339885f0c) \n",
    "* multi-timescale inspiration \n",
    "    * [Multi-timescale Representation Learning in LSTM Language Models](https://openreview.net/forum?id=9ITXiTrAoT)\n",
    "    * "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260a4926-e0bc-44d9-b97b-529e2fb11b33",
   "metadata": {},
   "source": [
    "[back to top](#outline_algos_framework) \n",
    "\n",
    "\n",
    "<div id='production_instruction'> </div>div> \n",
    "\n",
    "\n",
    "# **production instruction** notes on how things are currently ran \n",
    "\n",
    "* outline\n",
    "    * [clickhouse ---- algos_db](#clickhouse_algos_db)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3669a69b-4217-4e66-b60f-4c76e3b4c62c",
   "metadata": {},
   "source": [
    "<div id='clickhouse_algos_db'> </div>div> \n",
    "\n",
    "## clickhouse ---- algos_db"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5209c8e-636a-4a1e-b63c-77e3b6ee92f0",
   "metadata": {},
   "source": [
    "* setup commands (of course via chat gpt so maybe check some specific values if setting up as a script\n",
    "    * ```sudo apt update```\n",
    "    * ```sudo apt upgrade```\n",
    "    * ```sudo apt-key adv --keyserver keyserver.ubuntu.com --recv E0C56BD4```\n",
    "    * ```echo \"deb http://repo.yandex.ru/clickhouse/deb/stable/ main/\" | sudo tee /etc/apt/sources.list.d/clickhouse.list```\n",
    "    * ```sudo apt update```\n",
    "    * ```sudo apt install -y clickhouse-server clickhouse-client```\n",
    "    * ```sudo systemctl start clickhouse-server```\n",
    "    * ```sudo systemctl enable clickhouse-server```\n",
    "    * \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be713758-c50d-4536-af26-038d0b0ab322",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00bf06da-5b31-46f7-a839-d053c2b8bc69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e087e7-5844-45a2-9afc-b6da0f06205f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e23dc479-0841-4aad-a52b-eda0d7c45fa7",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "[back to top](#outline_algos_framework) \n",
    "\n",
    "<div id='live_functionality'> </div> \n",
    "\n",
    "* # short term **current** development and live functionality \n",
    "* outline \n",
    "    * [live_framework_adjustments](#live_framework_adjustments)  \n",
    "    * [async runner example](#async_runner_example) \n",
    "    * [EAORS integration](#EAORS_integration)\n",
    "    * [start up configurations](#start_up_configs)\n",
    "    * [`state_dict` split to `state` and `data` dicts](#state_dict_splitting) \n",
    "    * [liquidation map](#liquidation_map) \n",
    "    * [price volume profile](#price_volume_profile)\n",
    "    * [dash app](#dash_app)\n",
    "    * [continious live development scratch space](#live_dev_scratch_space) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce8e716-fb16-42a9-b889-ada6a46fbcfd",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div id='live_framework_adjustments'> </div> \n",
    "\n",
    "## live_framework_adjustments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af5cca1-d2f0-408b-9e6a-18dba4d8d0a2",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "!pip install ipdb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7335cd0d-5627-48a5-bd36-3927a9030004",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div id='async_runner_example'> </div> \n",
    "\n",
    "### async runner example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a170ed-b685-45f3-9822-a0f7efe88ad0",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if nb_params['live_trade_work'] == True:\n",
    "\n",
    "    import asyncio\n",
    "    import os\n",
    "    import time\n",
    "\n",
    "\n",
    "    class Runner:\n",
    "        def __init__(\n",
    "            self,\n",
    "            config,\n",
    "            logger,\n",
    "        ):\n",
    "            self.config = config\n",
    "            self.logger = logger\n",
    "            self.event_loop = asyncio.get_event_loop()\n",
    "\n",
    "        def run(self):\n",
    "            self._print_diagnostics()\n",
    "            failure_times = []\n",
    "            sleep_time = 10\n",
    "            while True:\n",
    "                try:\n",
    "                    asyncio.run(self._looper())\n",
    "                except:\n",
    "                    self.logger.exception(\"Encountered exception. Restarting\")\n",
    "                failure_times.append(time.time())\n",
    "                if len(failure_times) > 5 and (time.time() - failure_times[-5]) < (60 * 20):\n",
    "                    self.logger.error(\"Failed 5 times in 20 minutes: Aborting.\")\n",
    "                    break\n",
    "                self._cancel_tasks()\n",
    "                self.logger.info(f\"Sleeping for {sleep_time}\")\n",
    "                time.sleep(sleep_time)\n",
    "\n",
    "        def _print_diagnostics(self):\n",
    "            version_file = os.path.expanduser(\"~/VERSION.txt\")\n",
    "            if os.path.exists(version_file):\n",
    "                with open(version_file, \"r\") as f:\n",
    "                    version = f.read()\n",
    "                self.logger.info(f\"algos_dbs_dbs_dbs_dbs_dbs version={version.strip()}\")\n",
    "            else:\n",
    "                self.logger.info(f\"Unable to determine algos_dbs_dbs_dbs_dbs_dbs version information\")\n",
    "            self.logger.info(f\"Config file: {self.config}\")\n",
    "\n",
    "        def _cancel_tasks(self):\n",
    "            # TODO: Fairly sure this doesn't do anything since asyncio.run creates a new task each time\n",
    "            #     self.event_loop is the loop for the first run, but every subsequent run will be different\n",
    "            self.logger.warning(\"Killing all the tasks!\")\n",
    "            for task in asyncio.all_tasks(self.event_loop):\n",
    "                task.cancel()\n",
    "\n",
    "        async def _looper(self):\n",
    "            raise NotImplementedError(\"Child class must implement _looper function!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bbd4d55-3ece-42a2-80a4-bfd33d907fe9",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "* #### applying this to the algos framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa8a4f4-152c-4a7c-b299-6de831640936",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if nb_params['live_trade_work'] == True:\n",
    "    from algos.runner import Runner\n",
    "    from src.util import get_logger, get_secret\n",
    "\n",
    "\n",
    "    class AlgosFrameworkRunner(Runner):\n",
    "        async def _looper(self):\n",
    "            loop = asyncio.get_event_loop()\n",
    "            update_queue = asyncio.Queue(loop=loop)\n",
    "            order_queue = asyncio.Queue(loop=loop)\n",
    "\n",
    "            instruments = ['BTC-USDT']\n",
    "\n",
    "            # TODO: Dynamically load the exchanges\n",
    "            binance = Binance(\n",
    "                instrument=instruments,\n",
    "                api_key=get_secret(\"BINANCE_API_KEY_1\"),\n",
    "                api_secret=get_secret(\"BINANCE_SECRET_KEY_1\"),\n",
    "            )\n",
    "\n",
    "\n",
    "    # def get_args():\n",
    "    #     parser = argparse.ArgumentParser()\n",
    "    #     parser.add_argument(\n",
    "    #         \"-c\",\n",
    "    #         \"--config-dir\",\n",
    "    #         help=\"Path to directory to find config files\",\n",
    "    #         required=True,\n",
    "    #     )\n",
    "    #     parser.add_argument(\n",
    "    #         \"-i\", \"--instrument\", default=\"all\", help=\"Instruments to run for\"\n",
    "    #     )\n",
    "    #     parser.add_argument(\n",
    "    #         \"-s\", \"--sleep\", action=\"store_true\", help=\"Sleep for a random amount of time\"\n",
    "    #     )\n",
    "    #     return parser.parse_args()\n",
    "\n",
    "\n",
    "    # def main():\n",
    "\n",
    "\n",
    "    # args = get_args()\n",
    "    configs = []\n",
    "\n",
    "    logger = get_logger(f\"algos-framework\")\n",
    "\n",
    "    if args.sleep:\n",
    "        sleep_time = int(30 * random.random())\n",
    "        logger.info(f\"Sleeping for {sleep_time} seconds before launching\")\n",
    "        time.sleep(sleep_time)\n",
    "    runner = AlgosFrameworkRunner(configs, logger)\n",
    "    runner.run()\n",
    "\n",
    "\n",
    "    # if __name__ == \"__main__\":\n",
    "    #     main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a27cf090-6705-409d-9527-5a6c841879bb",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div id='EAORS_integration'> </div> \n",
    "\n",
    "### EAORS integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69010a4e-e666-41f0-add2-e688c49bac3d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if nb_params['live_trade_work'] == True:\n",
    "    instruments = ['BTC-BUSD']\n",
    "    \n",
    "    # TODO: Dynamically load the exchanges\n",
    "    binance = Binance(\n",
    "        instrument=instruments,\n",
    "        api_key=get_secret(\"BINANCE_API_KEY_1\"),\n",
    "        api_secret=get_secret(\"BINANCE_SECRET_KEY_1\"),\n",
    "    )\n",
    "\n",
    "    task = asyncio.create_task(binance.start())\n",
    "    \n",
    "    # error line # comment out this line to run the cell\n",
    "    res_place_order = await binance.place_order(instrument='BTC-BUSD',\n",
    "                                                size=0.0006295907660020986,\n",
    "                                                order_type=OrderType.LIMIT_ORDER,\n",
    "                                                direction=OrderDirection.BUY,\n",
    "                                                price = 23801.99,\n",
    "                                                time_in_force = None,\n",
    "                                                 )\n",
    "    \n",
    "print(f\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e9399b-cefe-49f9-952e-143eeb6506a2",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "* #### exploring EAORS responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c39131-b234-4ad7-a228-f1a87ebf87b2",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# binance.order_queue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe5aba6-058f-4002-9cf2-89702e0ea3c0",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# res_place_order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ae0ce5-2927-4927-9f9f-0f04ee67367a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# error line # comment out this line to run the cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6f5482-3b15-43de-9ed4-2d725d496e48",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if nb_params['live_trade_work'] == True:\n",
    "    res_place_order_sell = await binance.place_order(instrument='BTC-BUSD',\n",
    "                                                size=0.0006295907660020986,\n",
    "                                                order_type=OrderType.LIMIT_ORDER,\n",
    "                                                direction=OrderDirection.SELL,\n",
    "                                                price = 23800.05,\n",
    "                                                time_in_force = None,\n",
    "                                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542c4c54-bfbc-4abc-8a74-5cb9bb32c1de",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# res_place_order_sell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa62a05-50a2-4c2f-a010-16ed892d50d1",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# binance.order_queue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f489c7-d282-493f-955b-32c025e2e30c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if nb_params['live_trade_work'] == True:\n",
    "    res_get_balance = binance.get_balance()\n",
    "    res_get_balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa95aeb-f319-435b-accf-763d20ad5559",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if nb_params['live_trade_work'] == True:\n",
    "    res_get_orders = await binance.get_orders()\n",
    "    res_get_orders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507989f8-a1c7-4c5d-b379-85c17fc2d293",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "[back to top](#outline_algos_framework) \n",
    "\n",
    "<div id='state_dict_splitting'> </div> \n",
    "\n",
    "\n",
    "* ### `state_dict` split to `state` and `data` dicts\n",
    "    * finish [signal_revamp](#signal_revamp) above first, that way wer can get a fresh state dict going "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec556c8c-8952-48c4-afbf-cdd85a5adc49",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# feature_params = signal_dict['feature_params']\n",
    "# model_params = signal_dict['model_params']\n",
    "# model = signal_dict['model']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6726d1ff-9c27-41f8-aad9-f32d753a3aca",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "state_dict_fp = f\"{params['dirs']['data_dir']}live/ports/prod_1____BTC_USDT_trades_only_data/state_dict.pickle\"\n",
    "\n",
    "# OPENING\n",
    "with open(state_dict_fp, 'rb') as handle:\n",
    "     state_dict  = pickle.load(handle) #   <<<------- COPY PASTE DESIRED VARIABLE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b75463-d5d6-42f6-9a2b-8b00854a77bd",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "state_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a813a8-a770-4d3a-a128-97bce3e3a001",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "state_dict['LS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b307b6f-78b9-4128-8bf4-0b90fbc0a184",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "oos = ccxt_client.fetch_open_orders('BTCTUSD')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d92af5-803c-4585-9f16-1279f6cad338",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "[back to top](#outline_algos_framework) \n",
    "\n",
    "<div id='liquidation_map'> </div> \n",
    "\n",
    "* ### liquidation map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c9b86a-749f-4137-8565-338ab7b4ac79",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def load_liquidation_map(decay, window_decay, path = '/opt/shared/pchen/liquidation_maps/data/bitfinex_distrib_long/'):\n",
    "    \n",
    "    #l_maps = pd.read_csv(f\"/opt/shared/pchen//liquidation_maps/data/hourly_decay_{decay}_rol_wind_size_720_v3.csv\")\n",
    "    l_maps = pd.DataFrame()\n",
    "    l_maps = pd.read_csv(f\"{path}/hourly_decay_{decay}_rol_wind_size_{window_decay}.csv\", sep=';')\n",
    "    l_maps = l_maps.iloc[:,1:]\n",
    "    l_maps['date'] = pd.to_datetime(l_maps['date'])\n",
    "    l_maps.set_index('date', inplace = True)\n",
    "    l_maps.rename(columns={\"short_liquidation\": \"short_estimated_liquidation\", \"long_liquidation\": \"long_estimated_liquidation\"}, inplace = True)\n",
    "    \n",
    "    return l_maps\n",
    "\n",
    "l_maps = load_liquidation_map(0.1,720, path = '/opt/shared/pchen/liquidation_maps/data/bitfinex_distrib_long/bitfinex_2021/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7917cf2-07cd-4126-a508-8d7291f56081",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "l_maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ea0b10-bbe2-4a48-8457-3faee7971a5b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea3167e-1ffb-4ea1-8ca6-2354b3f73900",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# l_maps.iloc[:50].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c02683-c2fe-4f9e-b02d-1eb03c51ae0f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# l_maps.iloc[-50:].index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f402b56f-1c76-4883-9943-96af2617dbcd",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "[back to top](#outline_algos_framework) \n",
    "\n",
    "<div id='price_volume_profile'> </div> \n",
    "\n",
    "* ### price volume profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e893ad21-e09b-4e7a-bce5-fac6f1237569",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def calculate_liquidity_map(df, price_bin_width=1000):\n",
    "    \"\"\"\n",
    "    Calculates the trading volume within different price ranges.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: DataFrame with columns ['timestamp', 'price', 'amount']\n",
    "    - price_bin_width: Width of the price range to consider\n",
    "    \n",
    "    Returns:\n",
    "    A DataFrame containing the liquidity map\n",
    "    \"\"\"\n",
    "    # Calculate bins for price ranges\n",
    "    min_price = df['price'].min()\n",
    "    max_price = df['price'].max()\n",
    "    bins = np.arange(min_price, max_price + price_bin_width, price_bin_width)\n",
    "    \n",
    "    # Classify each transaction into a price bin and sum up the trading volume for each bin\n",
    "    df['price_bin'] = pd.cut(df['price'], bins, labels=(bins[:-1] + bins[1:]) / 2)\n",
    "    liquidity_map = df.groupby('price_bin')['amount'].sum().reset_index()\n",
    "    \n",
    "    return liquidity_map\n",
    "\n",
    "# Calculate liquidity map with a price bin width of 100\n",
    "liquidity_map = calculate_liquidity_map(df, price_bin_width=100)\n",
    "\n",
    "print(liquidity_map)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c9632cb-5957-4c43-ade0-f9871e1b0535",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "[back to top](#outline_algos_framework)\n",
    "\n",
    "<div id='dash_app'> </div> \n",
    "\n",
    "## dash app "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb2a160-c5a2-4b36-b35d-151b12a0551f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import dash\n",
    "import dash_core_components as dcc\n",
    "import dash_html_components as html\n",
    "from dash.dependencies import Input, Output\n",
    "\n",
    "app = dash.Dash(__name__)\n",
    "\n",
    "# Define your layout\n",
    "app.layout = html.Div([\n",
    "    dcc.Graph(id='live-plot'),\n",
    "    dcc.Interval(id='graph-update', interval=1*1000, n_intervals=0)  # Update every 1 second\n",
    "])\n",
    "\n",
    "# Define callback to update graph\n",
    "@app.callback(Output('live-plot', 'figure'),\n",
    "              [Input('graph-update', 'n_intervals')])\n",
    "def update_graph(n):\n",
    "    # Your function to generate a figure\n",
    "    fig = plot_framework(plot_requests=plot_requests,\n",
    "                         prices=vwap_dict['BTC-TUSD']['vwap'],\n",
    "                         framework_results=framework_results, \n",
    "                         downsample_n=20_000, \n",
    "                         ideal_bottoms=None, \n",
    "                         ideal_tops=None, )\n",
    "    return fig\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run_server(debug=True, port=6969)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93e420d-265e-4871-a4b1-60addbf6517d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plot_requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da24a009-32c3-4e20-8fec-31c02cc97eaf",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "[back to top](#outline_algos_framework)\n",
    "\n",
    "<div id='live_dev_scratch_space'> </div> \n",
    "\n",
    "\n",
    "## continious live development scratch space\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47860e1d-b12b-4e17-91d7-17b257211840",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "* ## adding leg group to positions table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa0d1d5-c7d6-4713-95a4-098a6560279d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "now = datetime.datetime.utcnow()\n",
    "leg_group_id = int(now.timestamp() * 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b87d0121-e696-4913-b21e-bb9f8a33d055",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "1688755358221  # bobs method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8c0e8d-4cbf-45bc-ac9b-8351fabf3623",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_table(client):\n",
    "    client.execute('''\n",
    "        CREATE TABLE IF NOT EXISTS algos.TempPositions2 AS algos.Positions\n",
    "    ''')\n",
    "create_table(ch_client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3574bcc-e3e9-4b4e-a37c-1e2030007f40",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "cutoff_time = \"2023-07-10 13:42:36\"\n",
    "query = f\"\"\"SELECT *\n",
    "FROM algos.Positions\n",
    "WHERE algo = 'prod_1____BTC_USDT_trades_only_data'     \n",
    "ORDER BY timestamp desc;\"\"\" \n",
    "#     timestamp <= '{cutoff_time}'\n",
    "\n",
    "# positions = ch_client.execute(query)\n",
    "positions = ch_client.query_dataframe(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8665528-0491-4522-bcd6-0bba6d953b7f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "positions['timestamp'] = pd.to_datetime(positions['timestamp'])\n",
    "positions['leg_group_id'] = positions['timestamp'].astype(int) / 10**6\n",
    "positions['leg_group_id'] = positions['leg_group_id'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c5be38-4bd4-44b7-a54b-296a10862ae1",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def insert_data(client, df):\n",
    "    # Assuming that columns of df correspond to those in the table\n",
    "    insert_query = 'INSERT INTO algos.Positions VALUES'\n",
    "\n",
    "    values = df.to_dict('records')\n",
    "    client.execute(insert_query, values)\n",
    "\n",
    "# Call the function to insert data\n",
    "insert_data(ch_client, positions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35425c37-a68f-41df-aa2a-ed4c3a698185",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# cutoff_time = \"2023-07-10 13:42:36\"\n",
    "# delete_query = f\"\"\"\n",
    "# ALTER TABLE algos.Positions\n",
    "# DELETE WHERE \n",
    "#     algo = 'prod_1____BTC_USDT_trades_only_data' AND\n",
    "#     timestamp <= '{cutoff_time}';\"\"\" \n",
    "\n",
    "# # positions = ch_client.execute(query)\n",
    "# num_deleted = ch_client.query_dataframe(delete_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8159056b-b253-4f2b-ba74-20bbf5be9c4f",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "* ## checking the mechanics of trading BTC with the \"initial short\" notional staying the same "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab1602f-3775-4128-8343-fbcff9149c05",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "asset_price = 30_000  \n",
    "asset_price = 24_000\n",
    "asset_price = 48_000\n",
    "asset_price = 24_000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f257b58-83c7-4263-afa3-31be04f89002",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "* #### with the portfolio (if it were trading actual long / short) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18028366-cb23-46ca-aa5d-54ac52db26e8",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "port_usd = 50\n",
    "port_usd = port_usd * 1.2 # short (asset goes down by 20%) \n",
    "port_usd = port_usd * 2   # long  (and asset goes up by 100% )\n",
    "port_usd = port_usd * 1.5 # short (asset goes down by 50% ) \n",
    "port_usd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11fb8d74-371c-427e-ab4c-6f33882f52f3",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "* #### portfolio how it is  being ran\n",
    "    * first, the actual spot part "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad744ad-d28e-4562-bc8a-017ab10a9b88",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "*  then the theoretical permenant short open on BTC for the same initial notional "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae8f6582-e1cb-4d54-8d61-1f773f033fbb",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# actual spot mechanics of trade ---- SPOT SIDE\n",
    "spot_usd = 50\n",
    "# spot_usd = spot_usd * 0.8 # short (asset goes down by 20%)   # commented out because there is no change (ALL IN USD during a short in the spot portfolio)\n",
    "spot_usd = (2*spot_usd) * 2   # long  (and asset goes up by 100% )\n",
    "# spot_usd = spot_usd * 1.5 # short (asset goes down by 50% )  # commented out because there is no change (ALL IN USD during a short in the spot portfolio)\n",
    "spot_usd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b79a64b1-fd7c-441f-a270-e349a5c1d687",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# actual spot mechanics of trade ---- SPOT SIDE\n",
    "initial_fake_short_value = 50\n",
    "fake_short_value = initial_fake_short_value * 1.2 # short (asset goes down by 20%)   # commented out because there is no change (ALL IN USD during a short in the spot portfolio)\n",
    "fake_short_value = initial_fake_short_value * 0.4   # long  (and asset goes up by 100% )\n",
    "fake_short_value = initial_fake_short_value * 1.2 # short (asset goes down by 50% )  # commented out because there is no change (ALL IN USD during a short in the spot portfolio)\n",
    "fake_short_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb6e7d8e-a1b6-49e0-98c9-dd0b2210c7f6",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "\n",
    "# \\#\n",
    "# \\# \n",
    "# ###START DEV: Avellaneda Stoikov Market Making "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad9501a-9d8e-439e-9e07-d50599cc87e0",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "[back to top](#outline) \n",
    "\n",
    "<div id='avellaneda_stoikov_mm'> </div> \n",
    "\n",
    "* ### avellaneda stoikov market making metric ---- feature generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37b1f51-bc16-4224-a772-ed2205496f13",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### definitions\n",
    "* `s`: This is the current market mid-price of BTC. You would typically get this value from the exchange's API.\n",
    "* `q`: This is the quantity of assets in the inventory of the base asset. It can be positive for long positions or negative for short positions. This would depend on your current inventory of BTC.\n",
    "* `sigma`: This is the market volatility. You could calculate this using historical price data. For BTC, you would typically calculate this from historical BTC price data. Volatility changes with market conditions and should be updated regularly.\n",
    "* `T`: This is the closing time, when the measurement period ends. This can be set based on your trading strategy. For example, if you plan to rebalance your inventory daily, you might set T=1.\n",
    "* `t`: This is the current time. If T is normalized to 1, t is a time fraction.\n",
    "* `gamma`: This is the inventory risk aversion parameter. This should be set based on your risk tolerance. A higher value implies that you are less willing to take on inventory risk.\n",
    "* `kappa`: This is the order book liquidity parameter. This could be estimated from the order book depth, which can usually be obtained from the exchange's API. A higher value implies that the order book is denser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9627aff4-bf4f-4caf-836e-37982298469b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class AvellanedaStoikov:\n",
    "    def __init__(self, s, q, sigma, T, t, gamma, kappa):\n",
    "        self.s = s\n",
    "        self.q = q\n",
    "        self.sigma = sigma\n",
    "        self.T = T\n",
    "        self.t = t\n",
    "        self.gamma = gamma\n",
    "        self.kappa = kappa\n",
    "\n",
    "    def calculate_reservation_price(self):\n",
    "        return self.s - self.gamma * self.sigma ** 2 * (self.T - self.t) * self.q\n",
    "\n",
    "    def calculate_optimal_spread(self):\n",
    "        term1 = self.sigma ** 2 * (self.T - self.t) + 2 / np.exp(self.kappa * (self.T - self.t)) - 2\n",
    "        term2 = self.gamma * self.sigma ** 2 * (self.T - self.t)\n",
    "        return np.sqrt(term1) + term2\n",
    "\n",
    "    def calculate_bid_ask(self):\n",
    "        res_price = self.calculate_reservation_price()\n",
    "        optimal_spread = self.calculate_optimal_spread()\n",
    "\n",
    "        bid_offer_price = res_price - optimal_spread / 2\n",
    "        ask_offer_price = res_price + optimal_spread / 2\n",
    "\n",
    "        return bid_offer_price, ask_offer_price\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b50503b-48b8-4c40-abc1-63ffc4fbd7b1",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "prices.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8191a1-f128-4108-8156-43283fdcd680",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "as_mm_df = pd.DataFrame(columns=['s', 'reserve_price', 'optimal_spread', 'q', 'sigma', 'T', 'gamma', 'kappa'], index=prices.index)\n",
    "as_mm_df['s'] = prices['vwap']\n",
    "as_mm_df['q'].iloc[0] = 0\n",
    "as_mm_df['T'] = 1\n",
    "as_mm_df['gamma'] = 1\n",
    "as_mm_df['kappa'] = 1\n",
    "\n",
    "\n",
    "as_mm_df.head(2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37cd01b6-ad39-4287-8afc-a0920a0abb20",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "db6f0486-eadd-4576-a2e3-9a52d4325405",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "the iterative calculation of q \n",
    "* this is needed each iteration... for now we are going to have to get it by looking at the change of the price over a certain time period "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a723c3c-e81e-4d5f-9dd9-7b8861c259b2",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### definitions\n",
    "* `s`: This is the current market mid-price of BTC. You would typically get this value from the exchange's API.\n",
    "* `q`: This is the quantity of assets in the inventory of the base asset. It can be positive for long positions or negative for short positions. This would depend on your current inventory of BTC.\n",
    "* `sigma`: This is the market volatility. You could calculate this using historical price data. For BTC, you would typically calculate this from historical BTC price data. Volatility changes with market conditions and should be updated regularly.\n",
    "* `T`: This is the closing time, when the measurement period ends. This can be set based on your trading strategy. For example, if you plan to rebalance your inventory daily, you might set T=1.\n",
    "* `t`: This is the current time. If T is normalized to 1, t is a time fraction.\n",
    "* `gamma`: This is the inventory risk aversion parameter. This should be set based on your risk tolerance. A higher value implies that you are less willing to take on inventory risk.\n",
    "* `kappa`: This is the order book liquidity parameter. This could be estimated from the order book depth, which can usually be obtained from the exchange's API. A higher value implies that the order book is denser."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f882b8-b80c-46fd-a5a4-552cdd3a9b3a",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# ###END DEV: Avellaneda Stoikov Market Making \n",
    "# \\#\n",
    "# \\# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b2db42-b9ca-4f3b-8941-64059e27cfbc",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "[back to top](#outline_algos_framework) \n",
    "\n",
    "<div id='copy_pasta'> </div> \n",
    "\n",
    "# copy pasta \n",
    "* #### no dev below this point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d24f70-1a23-431d-b96d-8a6929fcddcb",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# ### ONE LINE WRITE PICKLE\n",
    "#\n",
    "#\n",
    "import pickle; fp = '/home/pboehringer/source/alpha-crypto/notebooks/data/prices_binance_btc_usdt_2017-08-18_2021-04-19.pickle';  pickle.dump(start_end_tiple_list, open(fp, \"wb\"))\n",
    "\n",
    "  \n",
    "        \n",
    "# MULTI LINE \n",
    "#\n",
    "\n",
    "# ### SAVING AND OPENING \n",
    "#\n",
    "import pickle;\n",
    "var_name = f\"test_signal_dict____0_06_peaks____L1_loss\"  #   <<<------- COPY PASTE DESIRED VARIABLE HERE \n",
    "fp = f\"{params['dirs']['data_dir']}pickled_signal_dicts{var_name}.pickle\"\n",
    "\n",
    "# SAVING\n",
    "pickle.dump(test_signal_dict____0_06_peaks____L1_loss,  #   <<<------- COPY PASTE DESIRED VARIABLE HERE\n",
    "            open(fp, \"wb\")) \n",
    "\n",
    "# OPENING\n",
    "with open(fp, 'rb') as handle:\n",
    "     test_signal_dict____0_06_peaks____L1_loss = pickle.load(handle) #   <<<------- COPY PASTE DESIRED VARIABLE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85448c6d-fb06-4079-ba5f-f59482118e95",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div id='sql_console_copypasta'> </div>\n",
    "\n",
    "# sql console"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "797bcb41-594e-4406-96ba-b0c571001035",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "```sql\n",
    "DESCRIBE amberdata.Trades\n",
    "\n",
    "WITH\n",
    "     '2018-01-01' as startdate,\n",
    "     '2018-01-03' as enddate\n",
    "SELECT *\n",
    "FROM amberdata.Trades\n",
    "WHERE\n",
    "    exchange = 'binance'\n",
    "    AND instrument='btc_usdt'\n",
    "    AND date (timestamp) >= startdate\n",
    "    AND date (timestamp) < enddate  -- not inclusive so it can be used to gather large date ranges in a rolling fashion\n",
    "    ORDER BY timestamp\n",
    "\n",
    "SELECT MAX(timestamp)\n",
    "FROM amberdata.Trades\n",
    "WHERE\n",
    "    exchange = 'binance'\n",
    "    AND instrument='btc_usdt'\n",
    "\n",
    "\n",
    "SELECT MAX(timestamp)\n",
    "FROM amberdata.Trades\n",
    "WHERE\n",
    "    exchange = 'binance'\n",
    "    AND instrument='btc_usdt'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed919e6a-8328-4d1c-a4c4-530c1f3d6096",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "* ## copypasta: debubbing targets, could be useful later "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e434e5-3c76-42c6-9277-a405a643a214",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pickle; \n",
    "from algos.targets import (get_peak_and_valley_idxs_in_intervals, \n",
    "                            make_peaks_vallies_to_alternating_buy_sell_idxs,\n",
    "                            get_datetimes_for_buy_sell_idxs,\n",
    "                            make_targets_from_buy_sell_idxs)\n",
    "from algos.plotting import (plot_price_with_buy_sell_times_on_price) \n",
    "\n",
    "fp = '/home/pboehringer/source/alpha-crypto/notebooks/data/debug_del_later____prices_error_on_target_making.pickle'; \n",
    "fp = '/home/pboehringer/source/alpha-crypto/notebooks/data/debug_del_later____prices_error_on_target_making2.pickle'; \n",
    "\n",
    "with open(fp, 'rb') as handle:\n",
    "     test_prices = pickle.load(handle)\n",
    "\n",
    "series=test_prices\n",
    "min_peak_dist=25\n",
    "peak_prominence=0.055\n",
    "interval_len_in_steps=43200\n",
    "smoothing_window=None\n",
    "\n",
    "y_for_target_making_dtis, _ = make_targets(series=series,\n",
    "                                           min_peak_dist=min_peak_dist,\n",
    "                                           peak_prominence=peak_prominence,\n",
    "                                           interval_len_in_steps=interval_len_in_steps,\n",
    "                                           smoothing_window=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b8f4a4-b8b1-447c-8067-8eacad8864cf",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# ### ENDING CELL: \n",
    "#\n",
    "#\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
